{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zcD--o4TxkFf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "#from layers import LSTMLayer, WaveNet, TimeDistributedDense, TemporalConvolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "history_lengths = np.load(\"data/history_length.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "56CBbEkbxkFg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mp7AHBpYxkFg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56Maw5w5xn6Q",
        "outputId": "fd53edb3-7828-4ba7-c36f-960f0edae3ad"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn81WGF2xqZ3",
        "outputId": "92746340-2529-4143-e621-af79916423b5"
      },
      "outputs": [],
      "source": [
        "!ls -a /content/drive/MyDrive/recsys_data/rnn_product_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "doF07f2rxsGM"
      },
      "outputs": [],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo_71tbZxt87",
        "outputId": "ade801c6-bf99-48c2-ac2a-3ab034f34331"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/recsys_data/rnn_product_data/data.zip -d data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPIsccelOnbk",
        "outputId": "4b33a2b3-1ef2-4358-bb5c-41ecad51b8b0"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print(f'Running on a TPU w/{tpu.num_accelerators()[\"TPU\"]} cores')\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "9cs9HK1wTaMj"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TFDataReader2:\n",
        "    def __init__(self, data_dir):\n",
        "        # Define feature columns and label columns\n",
        "        self.feature_cols = [\n",
        "            'user_id',\n",
        "            'history_length',\n",
        "            'order_size_history',\n",
        "            'reorder_size_history',\n",
        "            'order_number_history',\n",
        "            'order_dow_history',\n",
        "            'order_hour_history',\n",
        "            'days_since_prior_order_history',\n",
        "        ]\n",
        "        # Load all numpy arrays\n",
        "        self.data = {}\n",
        "        for col in self.feature_cols:\n",
        "            self.data[col] = np.load(os.path.join(\n",
        "                data_dir, f'{col}.npy'), mmap_mode='r')\n",
        "            # if col in self.expand_cols:\n",
        "            # self.data[col] = self.data[col].reshape(-1,1)\n",
        "        # rint(self.data.keys())\n",
        "        # Create train/val split\n",
        "        total_size = len(next(iter(self.data.values())))\n",
        "        remainder = total_size % 512\n",
        "        for col in self.feature_cols:\n",
        "            self.data[col] = np.concatenate([self.data[col], np.zeros(\n",
        "                (512-remainder, *self.data[col].shape[1:]), dtype=self.data[col].dtype)], axis=0)\n",
        "        train_size = int(0.95 * total_size)\n",
        "\n",
        "        self.train_indices = np.arange(train_size)\n",
        "        self.val_indices = np.arange(train_size, total_size)\n",
        "        self.all_indices = np.arange(total_size + 512 - remainder)\n",
        "\n",
        "    def _process_features(self, original_features, is_test):\n",
        "\n",
        "        if not is_test:\n",
        "            history_lengths = original_features['history_length'] - 1\n",
        "        else:\n",
        "            # Create new features dictionary with augmented features\n",
        "            history_lengths = original_features['history_length']\n",
        "        features = {\n",
        "            # Copy original features\n",
        "            **original_features,\n",
        "\n",
        "            # Add augmented features\n",
        "            'next_order_dow': tf.gather(original_features['order_dow_history'], history_lengths),\n",
        "            'next_order_hour': tf.gather(original_features['order_hour_history'], history_lengths),\n",
        "            'days_since_prior_order': tf.gather(original_features['days_since_prior_order_history'], history_lengths),\n",
        "            'next_order_number': tf.gather(original_features['order_number_history'], history_lengths),\n",
        "            'history_length': history_lengths\n",
        "        }\n",
        "        # Adjust history length for non-test data\n",
        "        if is_test:\n",
        "            output = {}\n",
        "        else:\n",
        "            output = {'next_reorder_size': tf.cast(\n",
        "                tf.gather(\n",
        "                    # The tensor to gather from\n",
        "                    features['reorder_size_history'],\n",
        "                    # The indices (must be a scalar tensor or tensor of scalars)\n",
        "                    history_lengths\n",
        "                ),\n",
        "                dtype=tf.float32\n",
        "            ), 'next_order_size': tf.cast(\n",
        "                tf.gather(\n",
        "                    # The tensor to gather from\n",
        "                    features['order_size_history'],\n",
        "                    # The indices (must be a scalar tensor or tensor of scalars)\n",
        "                    history_lengths\n",
        "                ),\n",
        "                dtype=tf.float32\n",
        "            )}\n",
        "        return features, output\n",
        "\n",
        "    def _create_dataset(self, indices, shuffle=True, is_test=False):\n",
        "        # Create feature datasets\n",
        "        features_dict = {col: tf.cast(\n",
        "            self.data[col][indices], tf.int32) for col in self.feature_cols}\n",
        "\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(features_dict)\n",
        "        # Apply processing before batching\n",
        "        dataset = dataset.map(\n",
        "            lambda x: self._process_features(x, is_test),\n",
        "            num_parallel_calls=tf.data.AUTOTUNE\n",
        "        )\n",
        "        if shuffle:\n",
        "            dataset = dataset.shuffle(buffer_size=10000)\n",
        "\n",
        "        # Enable prefetching\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def get_train_dataset(self, batch_size):\n",
        "        dataset = self._create_dataset(self.train_indices, shuffle=True)\n",
        "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "        # for element in dataset.take(1):\n",
        "        # print(element[0])\n",
        "        # Process features after batching\n",
        "        # dataset = dataset.map(lambda x: self._process_features((x), is_test=False))\n",
        "        return dataset\n",
        "\n",
        "    def get_val_dataset(self, batch_size):\n",
        "        dataset = self._create_dataset(self.val_indices, shuffle=True)\n",
        "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "        # dataset = dataset.map(lambda x: self._process_features((x), is_test=False))\n",
        "        return dataset\n",
        "\n",
        "    def get_test_dataset(self, batch_size):\n",
        "        dataset = self._create_dataset(\n",
        "            self.all_indices, shuffle=False, is_test=True)\n",
        "        dataset = dataset.batch(batch_size, drop_remainder=False)\n",
        "        # dataset = dataset.map(lambda x: self._process_features((x), is_test=True))\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RXI4WcedxkFh"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, conv_kernel_size=3, rate=0.1, **kwargs):\n",
        "        super(TransformerEncoderBlock, self).__init__(**kwargs)\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Position-wise feed-forward network with convolution\n",
        "        self.conv1 = layers.Conv1D(filters=ff_dim, kernel_size=conv_kernel_size, activation='relu', padding='same')\n",
        "        self.conv2 = layers.Conv1D(filters=embed_dim, kernel_size=conv_kernel_size, activation='relu', padding='same')\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "\n",
        "    def call(self, inputs, training, mask=None):\n",
        "        # Self-attention layer\n",
        "        attn_output = self.att(inputs, inputs, inputs, attention_mask=mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.norm1(inputs + attn_output)\n",
        "        \n",
        "        # Feed-forward network with convolutional layers\n",
        "        ff_output = self.conv1(out1)\n",
        "        ff_output = self.conv2(ff_output)\n",
        "        ff_output = self.dropout2(ff_output, training=training)\n",
        "        out2 = self.norm2(out1 + ff_output)\n",
        "        return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.sequence_mask(\n",
        "            4, maxlen=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "UwpkzUEE8xtZ"
      },
      "outputs": [],
      "source": [
        "class CustomModel(tf.keras.Model):\n",
        "    def __init__(self, num_transformer_blocks, embed_dim=128, num_heads=4, ff_dim=256, **kwargs):\n",
        "        super(CustomModel, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_transformer_blocks = num_transformer_blocks\n",
        "\n",
        "        self.project_dense = layers.Dense(embed_dim, activation='relu')\n",
        "        # Embedding layers\n",
        "\n",
        "        # Dense layers for non-embedding features\n",
        "        # Positional encoding\n",
        "        self.positional_encoding = self._get_positional_encoding(\n",
        "            100, embed_dim=embed_dim)\n",
        "        # print(self.positional_encoding.numpy())\n",
        "        # Transformer encoder blocks\n",
        "        self.transformer_blocks = tf.keras.Sequential([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, ff_dim)\n",
        "            for _ in range(num_transformer_blocks)\n",
        "        ])\n",
        "\n",
        "        # Output layers\n",
        "        self.dense1 = layers.Dense(64, activation='relu')\n",
        "        self.dense2 = layers.Dense(1, activation='relu')\n",
        "        self.dense3 = layers.Dense(1, activation='relu')\n",
        "        self.mse_loss = tf.keras.losses.Huber(\n",
        "            reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "    def _get_positional_encoding(self, maxlen, embed_dim):\n",
        "        \"\"\"Generate positional encoding using TensorFlow operations.\"\"\"\n",
        "        pos = tf.range(maxlen, dtype=tf.float32)[:, tf.newaxis]  # (maxlen, 1)\n",
        "        i = tf.range(embed_dim, dtype=tf.float32)[\n",
        "            tf.newaxis, :]  # (1, embed_dim)\n",
        "        angle_rates = 1 / tf.pow(10000.0, (2 * (i//2)) /\n",
        "                                 tf.cast(embed_dim, tf.float32))\n",
        "        angle_rads = pos * angle_rates  # (maxlen, embed_dim)\n",
        "\n",
        "        # Apply sin to even indices, cos to odd indices\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat(\n",
        "            [sines, cosines], axis=-1)  # (maxlen, embed_dim)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]  # (1, maxlen, embed_dim)\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    @tf.function\n",
        "    def masked_mean_pooling(self, encoder_outputs, mask):\n",
        "        \"\"\"\n",
        "        Computes the mean of encoder outputs considering the mask.\n",
        "\n",
        "        Args:\n",
        "            encoder_outputs: Tensor of shape (batch_size, max_len, embedding_dim)\n",
        "            mask: Tensor of shape (batch_size, max_len), where 1 indicates valid tokens and 0 indicates padding\n",
        "        Returns:\n",
        "            context_vector: Tensor of shape (batch_size, embedding_dim)\n",
        "        \"\"\"\n",
        "        mask = tf.cast(mask, dtype=tf.float32)  # Convert mask to float\n",
        "        # Shape: (batch_size, max_len, 1)\n",
        "        mask = tf.expand_dims(mask, axis=-1)\n",
        "        masked_outputs = encoder_outputs * mask  # Zero-out padding embeddings\n",
        "\n",
        "        # Sum over the time steps\n",
        "        summed = tf.reduce_sum(masked_outputs, axis=1)\n",
        "        # Number of valid tokens per sample\n",
        "        lengths = tf.reduce_sum(mask, axis=1)\n",
        "\n",
        "        # Avoid division by zero\n",
        "        lengths = tf.maximum(lengths, tf.ones_like(lengths))\n",
        "        context_vector = summed / lengths  # Shape: (batch_size, embedding_dim)\n",
        "\n",
        "        return context_vector\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        user_id = inputs['user_id']\n",
        "\n",
        "        self.history_length = inputs['history_length']\n",
        "        # print(self.history_length.shape)\n",
        "        order_size_history = inputs['order_size_history']\n",
        "        reorder_size_history = inputs['reorder_size_history']\n",
        "        order_number_history = inputs['order_number_history']\n",
        "        order_dow_history = inputs['order_dow_history']\n",
        "        order_hour_history = inputs['order_hour_history']\n",
        "        days_since_prior_order_history = inputs['days_since_prior_order_history']\n",
        "\n",
        "        next_order_number = inputs['next_order_number']\n",
        "        next_order_dow = inputs['next_order_dow']\n",
        "        next_order_hour = inputs['next_order_hour']\n",
        "        days_since_prior_order = inputs['days_since_prior_order']\n",
        "\n",
        "        # print(product_embeddings.shape, aisle_embeddings.shape, department_embeddings.shape, is_none_float.shape, product_names.shape)\n",
        "\n",
        "        # Sequence data\n",
        "        order_dow_history_onehot = tf.one_hot(order_dow_history, 8)\n",
        "        order_hour_history_onehot = tf.one_hot(order_hour_history, 25)\n",
        "        days_since_prior_order_history_onehot = tf.one_hot(\n",
        "            days_since_prior_order_history, 31)\n",
        "        order_size_history_onehot = tf.one_hot(order_size_history, 60)\n",
        "        reorder_size_history_onehot = tf.one_hot(reorder_size_history, 50)\n",
        "        order_number_history_onehot = tf.one_hot(order_number_history, 101)\n",
        "\n",
        "        next_order_dow_onehot = tf.one_hot(next_order_dow, 8)\n",
        "        next_order_hour_onehot = tf.one_hot(next_order_hour, 25)\n",
        "        days_since_prior_order_onehot = tf.one_hot(days_since_prior_order, 31)\n",
        "        next_order_number_onehot = tf.one_hot(next_order_number, 101)\n",
        "\n",
        "        #print('one_hot', next_order_dow_onehot.shape)\n",
        "\n",
        "        order_dow_history_scalar = tf.expand_dims(\n",
        "            tf.cast(order_dow_history, tf.float32) / 8.0, 2)\n",
        "        order_hour_history_scalar = tf.expand_dims(\n",
        "            tf.cast(order_hour_history, tf.float32) / 25.0, 2)\n",
        "        days_since_prior_order_history_scalar = tf.expand_dims(\n",
        "            tf.cast(days_since_prior_order_history, tf.float32) / 31.0, 2)\n",
        "        order_size_history_scalar = tf.expand_dims(\n",
        "            tf.cast(order_size_history, tf.float32) / 60.0, 2)\n",
        "        reorder_size_history_scalar = tf.expand_dims(\n",
        "            tf.cast(reorder_size_history, tf.float32) / 50.0, 2)\n",
        "        order_number_history_scalar = tf.expand_dims(\n",
        "            tf.cast(order_number_history, tf.float32) / 100.0, 2)\n",
        "\n",
        "        next_order_dow_scalar = tf.expand_dims(\n",
        "            tf.cast(next_order_dow, tf.float32) / 8.0, 1)\n",
        "        next_order_hour_scalar = tf.expand_dims(\n",
        "            tf.cast(next_order_hour, tf.float32) / 25.0, 1)\n",
        "        days_since_prior_order_scalar = tf.expand_dims(\n",
        "            tf.cast(days_since_prior_order, tf.float32) / 31.0, 1)\n",
        "        next_order_number_scalar = tf.expand_dims(\n",
        "            tf.cast(next_order_number, tf.float32) / 100.0, 1)\n",
        "\n",
        "        #print('scalar', next_order_dow_scalar.shape)\n",
        "\n",
        "        outputs = tf.concat([\n",
        "            order_dow_history_onehot,\n",
        "            order_hour_history_onehot,\n",
        "            days_since_prior_order_history_onehot,\n",
        "            order_size_history_onehot,\n",
        "            reorder_size_history_onehot,\n",
        "            order_number_history_onehot,\n",
        "            order_dow_history_scalar,\n",
        "            order_hour_history_scalar,\n",
        "            days_since_prior_order_history_scalar,\n",
        "            order_size_history_scalar,\n",
        "            reorder_size_history_scalar,\n",
        "            order_number_history_scalar,\n",
        "        ], axis=2)\n",
        "\n",
        "        #print(outputs.shape)\n",
        "        outputs = self.project_dense(outputs)\n",
        "        batch_size = tf.shape(self.history_length)[0]\n",
        "        positional_encoding = tf.tile(\n",
        "            self.positional_encoding, [batch_size, 1, 1])\n",
        "\n",
        "        # print(positional_encoding.shape)\n",
        "        # Step 2: Create a range tensor and compare it against each history_length to create a mask\n",
        "        \n",
        "        \n",
        "        mask = tf.cast(tf.sequence_mask(\n",
        "            self.history_length, maxlen=100), tf.float32)\n",
        "\n",
        "        # Expand mask for embedding dimensions\n",
        "        expanded_mask = tf.expand_dims(mask, -1)\n",
        "        # print(mask.shape)\n",
        "        # Broadcast to the shape of positional_encoding\n",
        "        mask1 = tf.broadcast_to(expanded_mask, tf.shape(positional_encoding))\n",
        "        mask2 = tf.broadcast_to(expanded_mask, tf.shape(outputs))\n",
        "        # print(mask2.numpy)\n",
        "        # Step 4: Apply the mask to the positional encoding\n",
        "        masked_positional_encoding = positional_encoding * mask1\n",
        "        outputs *= mask2\n",
        "        # Step 5: Add the masked positional encoding to the outputs\n",
        "        outputs += masked_positional_encoding\n",
        "        outputs = self.transformer_blocks(outputs, training=training)\n",
        "\n",
        "        h = self.masked_mean_pooling(outputs, mask)\n",
        "        #print('h', h.shape)\n",
        "        h = tf.concat([\n",
        "            h,\n",
        "            next_order_dow_onehot,\n",
        "            next_order_hour_onehot,\n",
        "            days_since_prior_order_onehot,\n",
        "            next_order_number_onehot,\n",
        "            next_order_dow_scalar,\n",
        "            next_order_hour_scalar,\n",
        "            days_since_prior_order_scalar,\n",
        "            next_order_number_scalar,\n",
        "        ], axis=1)\n",
        "        # (batch_size, 100, embed_dim)\n",
        "        h1 = self.dense1(h)\n",
        "        # h = tf.keras.layers.TimeDistributed(self.dense1, name='hidden_states')(h)\n",
        "        next_order_size = self.dense2(h1)\n",
        "        # Pass through Dense layers\n",
        "\n",
        "        # final_states = tf.keras.layers.Lambda(lambda x: x, name='final_states')(final_states)\n",
        "        # final_predictions = tf.keras.layers.Lambda(lambda x: x, name='final_predictions')(final_predictions)\n",
        "        return {'final_states': h1, 'next_order_size': next_order_size}\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            # Pass both prediction and history_length to loss\n",
        "            loss = self.mse_loss(y['next_order_size'],\n",
        "                                 y_pred['next_order_size'])\n",
        "\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.compiled_metrics.update_state(\n",
        "            y['next_order_size'], y_pred['next_order_size'])\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results['loss'] = loss\n",
        "        return results\n",
        "\n",
        "    @tf.function\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        y_pred = self(x, training=False)\n",
        "        # Pass both prediction and history_length to loss\n",
        "        loss = self.mse_loss(y['next_order_size'],\n",
        "                                 y_pred['next_order_size']) \n",
        "        self.compiled_metrics.update_state(\n",
        "            y['next_order_size'], y_pred['next_order_size'])\n",
        "\n",
        "        # Return metrics and val_loss\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results['loss'] = loss\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "I_4Z20f7cZ6u"
      },
      "outputs": [],
      "source": [
        "reader = TFDataReader2('data')\n",
        "train_dataset = reader.get_train_dataset(128)\n",
        "val_dataset = reader.get_val_dataset(128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "YVCMF4NJ_yF9"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"checkpoints/conformer_ordersize/cp-{epoch:04d}.ckpt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "0kauFm0OednL"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    # Early stopping to prevent overfitting\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # Model checkpoint to save best model\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath='models/best_model.h5',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # Learning rate reduction on plateau\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # CSV logger\n",
        "    tf.keras.callbacks.CSVLogger(\n",
        "        'training_log.csv',\n",
        "        separator=',',\n",
        "        append=False\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=True,\n",
        "        save_freq='epoch',\n",
        "        verbose=1\n",
        ")\n",
        "\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "element = None\n",
        "for el in val_dataset.take(1):\n",
        "    element = el\n",
        "    \n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "nqpEpl3WeLRW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model = CustomModel(\n",
        "        num_transformer_blocks=3\n",
        "    )\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.legacy.Adam(0.001),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.AUC(name='auc')\n",
        "    ]\n",
        ")\n",
        "\n",
        "#model(element[0])\n",
        "#model.load_weights(\"models/best_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwll09q0W3Oa",
        "outputId": "249c4cc3-530b-41d2-8c27-92ac9525d4eb"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_dataset.repeat(),\n",
        "    validation_data=val_dataset,\n",
        "    epochs=40,\n",
        "    steps_per_epoch=500,\n",
        "    validation_steps=None,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,  # 0: silent, 1: progress bar, 2: one line per epoch\n",
        "    shuffle=True,  # Shuffle training data\n",
        "    max_queue_size=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "n0WRXgTg7Tyu"
      },
      "outputs": [],
      "source": [
        "model.load_weights(\n",
        "            f'models/best_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dataset = reader.get_test_dataset(512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "outputs = model.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "true_len = len(reader.data['user_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "4DAh8Oma7XGu"
      },
      "outputs": [],
      "source": [
        "np.save('pred_data/final_states.npy', outputs['final_states'][:true_len] )\n",
        "np.save('pred_data/pred_order_size.npy', outputs['next_order_size'][:true_len])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGVb4PvP745N"
      },
      "outputs": [],
      "source": [
        "for i in outputs['next_order_size']:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "RecSys",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
