{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zcD--o4TxkFf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "#from layers import LSTMLayer, WaveNet, TimeDistributedDense, TemporalConvolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "56CBbEkbxkFg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mp7AHBpYxkFg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56Maw5w5xn6Q",
        "outputId": "fd53edb3-7828-4ba7-c36f-960f0edae3ad"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn81WGF2xqZ3",
        "outputId": "92746340-2529-4143-e621-af79916423b5"
      },
      "outputs": [],
      "source": [
        "!ls -a /content/drive/MyDrive/recsys_data/rnn_product_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "doF07f2rxsGM"
      },
      "outputs": [],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo_71tbZxt87",
        "outputId": "ade801c6-bf99-48c2-ac2a-3ab034f34331"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/recsys_data/rnn_product_data/data.zip -d data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPIsccelOnbk",
        "outputId": "4b33a2b3-1ef2-4358-bb5c-41ecad51b8b0"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print(f'Running on a TPU w/{tpu.num_accelerators()[\"TPU\"]} cores')\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9cs9HK1wTaMj"
      },
      "outputs": [],
      "source": [
        "class TFDataReader2:\n",
        "    def __init__(self, data_dir):\n",
        "        # Define feature columns and label columns\n",
        "        self.feature_cols = [\n",
        "            'user_id', 'product_id', 'aisle_id', 'department_id',\n",
        "            'is_ordered_history', 'index_in_order_history',\n",
        "            'order_dow_history', 'order_hour_history',\n",
        "            'days_since_prior_order_history', 'order_size_history',\n",
        "            'reorder_size_history', 'order_number_history',\n",
        "            'history_length', 'product_name', 'product_name_length',\n",
        "            'product_embedding'\n",
        "        ]\n",
        "        self.expand_cols = ['user_id', 'product_id', 'aisle_id', 'department_id',\n",
        "                            'history_length', 'product_name_length', 'label']\n",
        "        self.label_cols = ['label']\n",
        "\n",
        "        # Load all numpy arrays\n",
        "        self.data = {}\n",
        "        for col in self.feature_cols + self.label_cols:\n",
        "            self.data[col] = np.load(os.path.join(\n",
        "                data_dir, f'{col}.npy'), mmap_mode='r')[:100000]\n",
        "            # if col in self.expand_cols:\n",
        "            # self.data[col] = self.data[col].reshape(-1,1)\n",
        "        # rint(self.data.keys())\n",
        "        # Create train/val split\n",
        "        total_size = len(next(iter(self.data.values())))\n",
        "        train_size = int(0.9 * total_size)\n",
        "\n",
        "        self.train_indices = np.arange(train_size)\n",
        "        self.val_indices = np.arange(train_size, total_size)\n",
        "        self.all_indices = np.arange(total_size)\n",
        "\n",
        "    def _process_features(self, original_features, is_test):\n",
        "\n",
        "        # Create new features dictionary with augmented features\n",
        "        features = {\n",
        "            # Copy original features\n",
        "            **original_features,\n",
        "\n",
        "            # Add augmented features\n",
        "            'order_dow_history': tf.roll(original_features['order_dow_history'], -1, axis=0),\n",
        "            'order_hour_history': tf.roll(original_features['order_hour_history'], -1, axis=0),\n",
        "            'days_since_prior_order_history': tf.roll(original_features['days_since_prior_order_history'], -1, axis=0),\n",
        "            'order_number_history': tf.roll(original_features['order_number_history'], -1, axis=0),\n",
        "            'is_none': tf.cast(tf.equal(original_features['product_id'],\n",
        "                                        tf.constant(0, dtype=tf.int32)), tf.int32)\n",
        "        }\n",
        "        print(features['is_none'].shape)\n",
        "        # Adjust history length for non-test data\n",
        "        if not is_test:\n",
        "            features['history_length'] = original_features['history_length'] - 1\n",
        "        else:\n",
        "            features['history_length'] = original_features['history_length']\n",
        "        return features, {'in_next_order': tf.cast(\n",
        "            tf.gather(\n",
        "                features['is_ordered_history'],  # The tensor to gather from\n",
        "                # The indices (must be a scalar tensor or tensor of scalars)\n",
        "                features['history_length']-1\n",
        "            ),\n",
        "            dtype=tf.float32\n",
        "        )}\n",
        "\n",
        "    def _create_dataset(self, indices, shuffle=True, is_test=False):\n",
        "        # Create feature datasets\n",
        "        features_dict = {col: tf.cast(\n",
        "            self.data[col][indices], tf.int32) for col in self.feature_cols}\n",
        "\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(features_dict)\n",
        "        # Apply processing before batching\n",
        "        dataset = dataset.map(\n",
        "            lambda x: self._process_features(x, is_test),\n",
        "            num_parallel_calls=tf.data.AUTOTUNE\n",
        "        )\n",
        "        if shuffle:\n",
        "            dataset = dataset.shuffle(buffer_size=10000)\n",
        "\n",
        "        # Enable prefetching\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def get_train_dataset(self, batch_size):\n",
        "        dataset = self._create_dataset(self.train_indices, shuffle=True)\n",
        "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "        # for element in dataset.take(1):\n",
        "        # print(element[0])\n",
        "        # Process features after batching\n",
        "        # dataset = dataset.map(lambda x: self._process_features((x), is_test=False))\n",
        "        return dataset\n",
        "\n",
        "    def get_val_dataset(self, batch_size):\n",
        "        dataset = self._create_dataset(self.val_indices, shuffle=True)\n",
        "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "        # dataset = dataset.map(lambda x: self._process_features((x), is_test=False))\n",
        "        return dataset\n",
        "\n",
        "    def get_test_dataset(self, batch_size):\n",
        "        dataset = self._create_dataset(\n",
        "            self.all_indices, shuffle=False, is_test=True)\n",
        "        dataset = dataset.batch(batch_size, drop_remainder=False)\n",
        "        # dataset = dataset.map(lambda x: self._process_features((x), is_test=True))\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RXI4WcedxkFh"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, conv_kernel_size=3, rate=0.1, **kwargs):\n",
        "        super(TransformerEncoderBlock, self).__init__(**kwargs)\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Position-wise feed-forward network with convolution\n",
        "        self.conv1 = layers.Conv1D(filters=ff_dim, kernel_size=conv_kernel_size, activation='relu', padding='same')\n",
        "        self.conv2 = layers.Conv1D(filters=embed_dim, kernel_size=conv_kernel_size, activation='relu', padding='same')\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs, training, mask=None):\n",
        "        # Self-attention layer\n",
        "        attn_output = self.att(inputs, inputs, inputs, attention_mask=mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.norm1(inputs + attn_output)\n",
        "        \n",
        "        # Feed-forward network with convolutional layers\n",
        "        ff_output = self.conv1(out1)\n",
        "        ff_output = self.conv2(ff_output)\n",
        "        ff_output = self.dropout2(ff_output, training=training)\n",
        "        out2 = self.norm2(out1 + ff_output)\n",
        "        return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "UwpkzUEE8xtZ"
      },
      "outputs": [],
      "source": [
        "class CustomModel(tf.keras.Model):\n",
        "    def __init__(self, num_transformer_blocks,embed_dim=256, num_heads=6, ff_dim=512 , **kwargs):\n",
        "        super(CustomModel, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_transformer_blocks = num_transformer_blocks\n",
        "\n",
        "        self.project_dense = layers.Dense(embed_dim, activation='relu')\n",
        "        # Embedding layers\n",
        "        self.product_embeddings = layers.Embedding(50000, embed_dim, name='product_embeddings')\n",
        "        self.user_embeddings = layers.Embedding(207000, embed_dim, name='user_embeddings')\n",
        "        self.aisle_embeddings = layers.Embedding(250, 50, name='aisle_embeddings')\n",
        "        self.department_embeddings = layers.Embedding(50, 10, name='department_embeddings')\n",
        "        \n",
        "        # Dense layers for non-embedding features\n",
        "        self.product_name_dense = layers.Dense(100, activation='relu')\n",
        "        \n",
        "        # Positional encoding\n",
        "        self.positional_encoding = self._get_positional_encoding(100, embed_dim)\n",
        "        #print(self.positional_encoding.numpy())\n",
        "        # Transformer encoder blocks\n",
        "        self.transformer_blocks = tf.keras.Sequential([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, ff_dim) \n",
        "            for _ in range(num_transformer_blocks)\n",
        "        ])\n",
        "        \n",
        "        # Output layers\n",
        "        self.dense1 = layers.Dense(128, activation='relu')\n",
        "        self.dense2 = layers.Dense(1, activation='sigmoid')\n",
        "        self.bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    def _get_positional_encoding(self, maxlen, embed_dim):\n",
        "        \"\"\"Generate positional encoding using TensorFlow operations.\"\"\"\n",
        "        pos = tf.range(maxlen, dtype=tf.float32)[:, tf.newaxis]  # (maxlen, 1)\n",
        "        i = tf.range(embed_dim, dtype=tf.float32)[tf.newaxis, :]  # (1, embed_dim)\n",
        "        angle_rates = 1 / tf.pow(10000.0, (2 * (i//2)) / tf.cast(embed_dim, tf.float32))\n",
        "        angle_rads = pos * angle_rates  # (maxlen, embed_dim)\n",
        "        \n",
        "        # Apply sin to even indices, cos to odd indices\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "        \n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)  # (maxlen, embed_dim)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]  # (1, maxlen, embed_dim)\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "    @tf.function\n",
        "    def masked_mean_pooling(self, encoder_outputs, mask):\n",
        "        \"\"\"\n",
        "        Computes the mean of encoder outputs considering the mask.\n",
        "\n",
        "        Args:\n",
        "            encoder_outputs: Tensor of shape (batch_size, max_len, embedding_dim)\n",
        "            mask: Tensor of shape (batch_size, max_len), where 1 indicates valid tokens and 0 indicates padding\n",
        "        Returns:\n",
        "            context_vector: Tensor of shape (batch_size, embedding_dim)\n",
        "        \"\"\"\n",
        "        mask = tf.cast(mask, dtype=tf.float32)  # Convert mask to float\n",
        "        mask = tf.expand_dims(mask, axis=-1)   # Shape: (batch_size, max_len, 1)\n",
        "        masked_outputs = encoder_outputs * mask  # Zero-out padding embeddings\n",
        "\n",
        "        summed = tf.reduce_sum(masked_outputs, axis=1)  # Sum over the time steps\n",
        "        lengths = tf.reduce_sum(mask, axis=1)           # Number of valid tokens per sample\n",
        "\n",
        "        # Avoid division by zero\n",
        "        lengths = tf.maximum(lengths, tf.ones_like(lengths))\n",
        "        context_vector = summed / lengths  # Shape: (batch_size, embedding_dim)\n",
        "\n",
        "        return context_vector\n",
        "    def call(self,inputs):\n",
        "        user_id = inputs['user_id']\n",
        "        product_id = inputs['product_id']\n",
        "        aisle_id = inputs['aisle_id']\n",
        "        department_id = inputs['department_id']\n",
        "        is_none = inputs['is_none']\n",
        "        self.history_length = inputs['history_length'] - 1\n",
        "        #print(self.history_length.shape)\n",
        "        is_ordered_history = inputs['is_ordered_history']\n",
        "        index_in_order_history = inputs['index_in_order_history']\n",
        "        order_dow_history = inputs['order_dow_history']\n",
        "        order_hour_history = inputs['order_hour_history']\n",
        "        days_since_prior_order_history = inputs['days_since_prior_order_history']\n",
        "        order_size_history = inputs['order_size_history']\n",
        "        reorder_size_history = inputs['reorder_size_history']\n",
        "        order_number_history = inputs['order_number_history']\n",
        "        product_name = inputs['product_name']\n",
        "\n",
        "        product_names = tf.one_hot(product_name, 2532)\n",
        "        #print(product_names.shape)\n",
        "        product_names = tf.reduce_max(product_names, axis=1)\n",
        "        #print(product_names.shape)\n",
        "        product_names = self.product_name_dense(product_names)\n",
        "        #print(product_names.shape)\n",
        "\n",
        "        is_none_float = tf.cast(tf.expand_dims(is_none, 1), tf.float32)\n",
        "\n",
        "        product_embeddings = self.product_embeddings(product_id)\n",
        "        aisle_embeddings = self.aisle_embeddings(aisle_id)\n",
        "        department_embeddings = self.department_embeddings(department_id)\n",
        "\n",
        "        #print(product_embeddings.shape, aisle_embeddings.shape, department_embeddings.shape, is_none_float.shape, product_names.shape)\n",
        "        x_product = tf.concat([\n",
        "            product_embeddings,\n",
        "            aisle_embeddings,\n",
        "            department_embeddings,\n",
        "            is_none_float,\n",
        "            product_names\n",
        "        ], axis=1)\n",
        "        x_product = tf.tile(tf.expand_dims(x_product, 1), (1, 100, 1))\n",
        "\n",
        "        # User data\n",
        "        user_embeddings = self.user_embeddings(user_id)\n",
        "        x_user = tf.tile(tf.expand_dims(user_embeddings, 1), (1, 100, 1))\n",
        "\n",
        "        # Sequence data\n",
        "        is_ordered_history_onehot = tf.one_hot(is_ordered_history, 2)\n",
        "        index_in_order_history_onehot = tf.one_hot(index_in_order_history, 20)\n",
        "        order_dow_history_onehot = tf.one_hot(order_dow_history, 8)\n",
        "        order_hour_history_onehot = tf.one_hot(order_hour_history, 25)\n",
        "        days_since_prior_order_history_onehot = tf.one_hot(days_since_prior_order_history, 31)\n",
        "        order_size_history_onehot = tf.one_hot(order_size_history, 60)\n",
        "        reorder_size_history_onehot = tf.one_hot(reorder_size_history, 50)\n",
        "        order_number_history_onehot = tf.one_hot(order_number_history, 101)\n",
        "\n",
        "        index_in_order_history_scalar = tf.expand_dims(tf.cast(index_in_order_history, tf.float32) / 20.0, 2)\n",
        "        order_dow_history_scalar = tf.expand_dims(tf.cast(order_dow_history, tf.float32) / 8.0, 2)\n",
        "        order_hour_history_scalar = tf.expand_dims(tf.cast(order_hour_history, tf.float32) / 25.0, 2)\n",
        "        days_since_prior_order_history_scalar = tf.expand_dims(tf.cast(days_since_prior_order_history, tf.float32) / 31.0, 2)\n",
        "        order_size_history_scalar = tf.expand_dims(tf.cast(order_size_history, tf.float32) / 60.0, 2)\n",
        "        reorder_size_history_scalar = tf.expand_dims(tf.cast(reorder_size_history, tf.float32) / 50.0, 2)\n",
        "        order_number_history_scalar = tf.expand_dims(tf.cast(order_number_history, tf.float32) / 100.0, 2)\n",
        "\n",
        "        x_history = tf.concat([\n",
        "            is_ordered_history_onehot,\n",
        "            index_in_order_history_onehot,\n",
        "            order_dow_history_onehot,\n",
        "            order_hour_history_onehot,\n",
        "            days_since_prior_order_history_onehot,\n",
        "            order_size_history_onehot,\n",
        "            reorder_size_history_onehot,\n",
        "            order_number_history_onehot,\n",
        "            index_in_order_history_scalar,\n",
        "            order_dow_history_scalar,\n",
        "            order_hour_history_scalar,\n",
        "            days_since_prior_order_history_scalar,\n",
        "            order_size_history_scalar,\n",
        "            reorder_size_history_scalar,\n",
        "            order_number_history_scalar,\n",
        "        ], axis=2)\n",
        "\n",
        "        outputs = tf.concat([x_history, x_product, x_user], axis=2)\n",
        "        \n",
        "        print(self.history_length.numpy())\n",
        "        outputs = self.project_dense(outputs)\n",
        "        batch_size = tf.shape(self.history_length)[0]\n",
        "        positional_encoding = tf.tile(self.positional_encoding, [batch_size, 1, 1])\n",
        "\n",
        "        #print(positional_encoding.shape)\n",
        "        # Step 2: Create a range tensor and compare it against each history_length to create a mask\n",
        "        mask = tf.cast(tf.sequence_mask(self.history_length, maxlen=100), tf.float32)\n",
        "        print(mask.numpy())\n",
        "\n",
        "        expanded_mask = tf.expand_dims(mask, -1)  # Expand mask for embedding dimensions\n",
        "        #print(mask.shape)\n",
        "        mask1 = tf.broadcast_to(expanded_mask, tf.shape(positional_encoding))  # Broadcast to the shape of positional_encoding\n",
        "        mask2 = tf.broadcast_to(expanded_mask, tf.shape(outputs))\n",
        "        #print(mask2.numpy)\n",
        "        # Step 4: Apply the mask to the positional encoding\n",
        "        masked_positional_encoding = positional_encoding * mask1\n",
        "        outputs *= mask2\n",
        "        # Step 5: Add the masked positional encoding to the outputs\n",
        "        outputs += masked_positional_encoding\n",
        "        outputs = self.transformer_blocks(outputs)\n",
        "        \n",
        "        h = self.masked_mean_pooling(outputs, mask)\n",
        "          # (batch_size, 100, embed_dim)\n",
        "        h1 = self.dense1(h)\n",
        "        #h = tf.keras.layers.TimeDistributed(self.dense1, name='hidden_states')(h)\n",
        "        final_predictions = self.dense2(h1)\n",
        "        # Pass through Dense layers\n",
        "\n",
        "\n",
        "        # final_states = tf.keras.layers.Lambda(lambda x: x, name='final_states')(final_states)\n",
        "        # final_predictions = tf.keras.layers.Lambda(lambda x: x, name='final_predictions')(final_predictions)\n",
        "        return { 'final_states': h1, 'in_next_order': final_predictions}\n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        history_length = x['history_length']\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            # Pass both prediction and history_length to loss\n",
        "            loss = self.bce_loss(y['in_next_order'], y_pred['in_next_order'])\n",
        "\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.compiled_metrics.update_state(y['in_next_order'], y_pred['in_next_order'])\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results['loss'] = loss\n",
        "        return results\n",
        "    @tf.function\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "        history_length = x['history_length']\n",
        "\n",
        "        y_pred = self(x, training=False)\n",
        "        # Pass both prediction and history_length to loss\n",
        "        loss = self.bce_loss(y['in_next_order'], y_pred['in_next_order'])\n",
        "\n",
        "        self.compiled_metrics.update_state(y['in_next_order'], y_pred['in_next_order'])\n",
        "\n",
        "        # Return metrics and val_loss\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results['loss'] = loss\n",
        "        return results\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YLw5C3QeRRkV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_4Z20f7cZ6u"
      },
      "outputs": [],
      "source": [
        "reader = TFDataReader2('data')\n",
        "train_dataset = reader.get_train_dataset(128)\n",
        "val_dataset = reader.get_val_dataset(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YVCMF4NJ_yF9"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/recsys_data/checkpoints/rnn_products/cp-{epoch:04d}.ckpt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0kauFm0OednL"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    # Early stopping to prevent overfitting\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # Model checkpoint to save best model\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath='models/best_model.h5',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # Learning rate reduction on plateau\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # CSV logger\n",
        "    tf.keras.callbacks.CSVLogger(\n",
        "        'training_log.csv',\n",
        "        separator=',',\n",
        "        append=False\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=True,\n",
        "        save_freq='epoch',\n",
        "        verbose=1\n",
        ")\n",
        "\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "element = None\n",
        "for el in val_dataset.take(1):\n",
        "    element = el\n",
        "    \n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(element[0]['history_length'])\n",
        "print(element[0]['is_ordered_history'])\n",
        "print(element[1]['in_next_order'].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqpEpl3WeLRW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model = CustomModel(\n",
        "        num_transformer_blocks=3\n",
        "    )\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "        tf.keras.metrics.Precision(name='precision')\n",
        "    ]\n",
        ")\n",
        "model(element[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwll09q0W3Oa",
        "outputId": "249c4cc3-530b-41d2-8c27-92ac9525d4eb"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=None,\n",
        "    validation_steps=None,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,  # 0: silent, 1: progress bar, 2: one line per epoch\n",
        "    shuffle=True,  # Shuffle training data\n",
        "    max_queue_size=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "n0WRXgTg7Tyu"
      },
      "outputs": [],
      "source": [
        "model.save_weights(\n",
        "            f'models/epoch_1.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DAh8Oma7XGu"
      },
      "outputs": [],
      "source": [
        "eval_results = model.evaluate(val_dataset, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGVb4PvP745N"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "RecSys",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
