{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zcD--o4TxkFf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#from layers import LSTMLayer, WaveNet, TimeDistributedDense, TemporalConvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "56CBbEkbxkFg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mp7AHBpYxkFg"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56Maw5w5xn6Q",
    "outputId": "fd53edb3-7828-4ba7-c36f-960f0edae3ad"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yn81WGF2xqZ3",
    "outputId": "92746340-2529-4143-e621-af79916423b5"
   },
   "outputs": [],
   "source": [
    "!ls -a /content/drive/MyDrive/recsys_data/rnn_product_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doF07f2rxsGM"
   },
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bo_71tbZxt87",
    "outputId": "ade801c6-bf99-48c2-ac2a-3ab034f34331"
   },
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/recsys_data/rnn_product_data/data.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPIsccelOnbk",
    "outputId": "4b33a2b3-1ef2-4358-bb5c-41ecad51b8b0"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "  print(f'Running on a TPU w/{tpu.num_accelerators()[\"TPU\"]} cores')\n",
    "except ValueError:\n",
    "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9cs9HK1wTaMj"
   },
   "outputs": [],
   "source": [
    "class TFDataReader2:\n",
    "    def __init__(self, data_dir):\n",
    "        # Define feature columns and label columns\n",
    "        self.feature_cols = [\n",
    "            'user_id',\n",
    "            'history_length',\n",
    "            'order_size_history',\n",
    "            'reorder_size_history',\n",
    "            'order_number_history',\n",
    "            'order_dow_history',\n",
    "            'order_hour_history',\n",
    "            'days_since_prior_order_history',\n",
    "        ]\n",
    "        self.expand_cols = ['user_id', 'product_id', 'aisle_id', 'department_id',\n",
    "            'history_length', 'product_name_length','label']\n",
    "\n",
    "        # Load all numpy arrays\n",
    "        self.data = {}\n",
    "        for col in self.feature_cols:\n",
    "            self.data[col] = np.load(os.path.join(data_dir, f'{col}.npy'), mmap_mode='r')\n",
    "            #if col in self.expand_cols:\n",
    "                #self.data[col] = self.data[col].reshape(-1,1)\n",
    "        #rint(self.data.keys())\n",
    "        # Create train/val split\n",
    "        total_size = len(next(iter(self.data.values())))\n",
    "        remainder = total_size % 512\n",
    "        for col in self.feature_cols:\n",
    "            self.data[col] = np.concatenate([self.data[col], np.zeros((512-remainder, *self.data[col].shape[1:]), dtype=self.data[col].dtype)], axis=0)\n",
    "        train_size = int(0.9 * total_size)\n",
    "\n",
    "        self.train_indices = np.arange(train_size)\n",
    "        self.val_indices = np.arange(train_size, total_size)\n",
    "        self.all_indices = np.arange(total_size + 512 - remainder)\n",
    "\n",
    "    def _process_features(self, original_features, is_test):\n",
    "\n",
    "        # Create new features dictionary with augmented features\n",
    "        features = {\n",
    "            # Copy original features\n",
    "            **original_features,\n",
    "\n",
    "            # Add augmented features\n",
    "            'order_dow_history': tf.roll(original_features['order_dow_history'], -1, axis=0),\n",
    "            'order_hour_history': tf.roll(original_features['order_hour_history'], -1, axis=0),\n",
    "            'days_since_prior_order_history': tf.roll(original_features['days_since_prior_order_history'], -1, axis=0),\n",
    "            'order_number_history': tf.roll(original_features['order_number_history'], -1, axis=0),\n",
    "        }\n",
    "        # Adjust history length for non-test data\n",
    "        if not is_test:\n",
    "            features['history_length'] = original_features['history_length'] - 1\n",
    "        else:\n",
    "            features['history_length'] = original_features['history_length']\n",
    "        return features, {'next_reorder_size': tf.cast(tf.roll(original_features['reorder_size_history'], -1, axis=0), dtype=tf.float32)}\n",
    "\n",
    "    def _create_dataset(self, indices, shuffle=True, is_test=False):\n",
    "        # Create feature datasets\n",
    "        features_dict = {col: tf.cast(self.data[col][indices], tf.int32) for col in self.feature_cols}\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(features_dict)\n",
    "        # Apply processing before batching\n",
    "        dataset = dataset.map(\n",
    "            lambda x: self._process_features(x, is_test),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=10000)\n",
    "\n",
    "        # Enable prefetching\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def get_train_dataset(self, batch_size):\n",
    "        dataset = self._create_dataset(self.train_indices, shuffle=True)\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        #for element in dataset.take(1):\n",
    "          #print(element[0])\n",
    "        # Process features after batching\n",
    "        #dataset = dataset.map(lambda x: self._process_features((x), is_test=False))\n",
    "        return dataset\n",
    "\n",
    "    def get_val_dataset(self, batch_size):\n",
    "        dataset = self._create_dataset(self.val_indices, shuffle=True)\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        #dataset = dataset.map(lambda x: self._process_features((x), is_test=False))\n",
    "        return dataset\n",
    "\n",
    "    def get_test_dataset(self, batch_size):\n",
    "        dataset = self._create_dataset(self.all_indices, shuffle=False, is_test=True)\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=False)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        #dataset = dataset.map(lambda x: self._process_features((x), is_test=True))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "S7dsk2hHxkFi"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sequence_rmse_loss(y, y_hat, sequence_lengths, max_sequence_length=100, eps=1e-7):\n",
    "    # Ensure y is float32 for calculations\n",
    "    #y = tf.cast(y, tf.float32)\n",
    "    #print('casted', y.shape)\n",
    "    # Clip predictions to avoid NaNs in log calculations\n",
    "    square_loss = tf.square(y-y_hat)\n",
    "    #print('clipped', y_hat.shape)\n",
    "    # Create a sequence mask based on sequence lengths\n",
    "    sequence_mask = tf.cast(tf.sequence_mask(sequence_lengths, maxlen=max_sequence_length), tf.float32)\n",
    "    #print(\"mask created\", sequence_mask.shape)\n",
    "    # Apply the sequence mask to the  losses\n",
    "    # Compute the average log loss\n",
    "    avg_sq_loss = tf.reduce_sum(square_loss*sequence_mask) / tf.cast(tf.reduce_sum(sequence_lengths), tf.float32)\n",
    "    #print(\"reduced\")\n",
    "    return tf.sqrt(avg_sq_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "UwpkzUEE8xtZ"
   },
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(self, lstm_size, **kwargs):\n",
    "        super(CustomModel, self).__init__(**kwargs)\n",
    "        self.lstm_size = lstm_size\n",
    "        self.lstm_layer = tf.keras.layers.GRU(lstm_size, return_sequences=True, dropout=0.0)\n",
    "        self.dense1 = tf.keras.layers.Dense(50, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    def call(self,inputs):\n",
    "        user_id = inputs['user_id']\n",
    "        self.history_length = inputs['history_length']\n",
    "\n",
    "        order_size_history = inputs['order_size_history']\n",
    "        reorder_size_history = inputs['reorder_size_history']\n",
    "        order_number_history = inputs['order_number_history']\n",
    "        order_dow_history = inputs['order_dow_history']\n",
    "        order_hour_history = inputs['order_hour_history']\n",
    "        days_since_prior_order_history = inputs['days_since_prior_order_history']\n",
    "\n",
    "        \n",
    "\n",
    "        #print(product_embeddings.shape, aisle_embeddings.shape, department_embeddings.shape, is_none_float.shape, product_names.shape)\n",
    "\n",
    "        # Sequence data\n",
    "        order_dow_history_onehot = tf.one_hot(order_dow_history, 8)\n",
    "        order_hour_history_onehot = tf.one_hot(order_hour_history, 25)\n",
    "        days_since_prior_order_history_onehot = tf.one_hot(days_since_prior_order_history, 31)\n",
    "        order_size_history_onehot = tf.one_hot(order_size_history, 60)\n",
    "        reorder_size_history_onehot = tf.one_hot(reorder_size_history, 50)\n",
    "        order_number_history_onehot = tf.one_hot(order_number_history, 101)\n",
    "\n",
    "\n",
    "\n",
    "        order_dow_history_scalar = tf.expand_dims(tf.cast(order_dow_history, tf.float32) / 8.0, 2)\n",
    "        order_hour_history_scalar = tf.expand_dims(tf.cast(order_hour_history, tf.float32) / 25.0, 2)\n",
    "        days_since_prior_order_history_scalar = tf.expand_dims(tf.cast(days_since_prior_order_history, tf.float32) / 31.0, 2)\n",
    "        order_size_history_scalar = tf.expand_dims(tf.cast(order_size_history, tf.float32) / 60.0, 2)\n",
    "        reorder_size_history_scalar = tf.expand_dims(tf.cast(reorder_size_history, tf.float32) / 50.0, 2)\n",
    "        order_number_history_scalar = tf.expand_dims(tf.cast(order_number_history, tf.float32) / 100.0, 2)\n",
    "\n",
    "\n",
    "\n",
    "        outputs = tf.concat([\n",
    "            order_dow_history_onehot,\n",
    "            order_hour_history_onehot,\n",
    "            days_since_prior_order_history_onehot,\n",
    "            order_size_history_onehot,\n",
    "            reorder_size_history_onehot,\n",
    "            order_number_history_onehot,\n",
    "            order_dow_history_scalar,\n",
    "            order_hour_history_scalar,\n",
    "            days_since_prior_order_history_scalar,\n",
    "            order_size_history_scalar,\n",
    "            reorder_size_history_scalar,\n",
    "            order_number_history_scalar,\n",
    "        ], axis=2)\n",
    "\n",
    "\n",
    "        h = self.lstm_layer(outputs)\n",
    "        h = tf.concat([h,outputs], axis=-1)\n",
    "        h = tf.keras.layers.TimeDistributed(self.dense1, name='hidden_states')(h)\n",
    "        y_hat = tf.keras.layers.TimeDistributed(self.dense2)(h)\n",
    "        y_hat = tf.squeeze(y_hat, axis=-1)\n",
    "        \n",
    "        final_temporal_idx = tf.stack([tf.range(tf.shape(self.history_length)[0]), self.history_length - 1], axis=1)\n",
    "        final_states = tf.gather_nd(h, final_temporal_idx)\n",
    "        final_predictions = tf.gather_nd(y_hat, final_temporal_idx)\n",
    "        # final_states = tf.keras.layers.Lambda(lambda x: x, name='final_states')(final_states)\n",
    "        # final_predictions = tf.keras.layers.Lambda(lambda x: x, name='final_predictions')(final_predictions)\n",
    "        return {'next_reorder_size': y_hat, 'final_states': final_states, 'final_predictions': final_predictions}\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        history_length = x['history_length']\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            # Pass both prediction and history_length to loss\n",
    "            loss = sequence_rmse_loss(y['next_reorder_size'], y_pred['next_reorder_size'], history_length, 100)\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(y['next_reorder_size'], y_pred['next_reorder_size'])\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results['loss'] = loss\n",
    "        return results\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        history_length = x['history_length']\n",
    "\n",
    "        y_pred = self(x, training=False)\n",
    "        # Pass both prediction and history_length to loss\n",
    "        loss = sequence_rmse_loss(y['next_reorder_size'], y_pred['next_reorder_size'], history_length, 100)\n",
    "\n",
    "        self.compiled_metrics.update_state(y['next_reorder_size'], y_pred['next_reorder_size'])\n",
    "\n",
    "        # Return metrics and val_loss\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results['loss'] = loss\n",
    "        return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLw5C3QeRRkV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "I_4Z20f7cZ6u"
   },
   "outputs": [],
   "source": [
    "reader = TFDataReader2('data')\n",
    "train_dataset = reader.get_train_dataset(128)\n",
    "val_dataset = reader.get_val_dataset(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "YVCMF4NJ_yF9"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints/cp-{epoch:04d}.ckpt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "0kauFm0OednL"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    # Early stopping to prevent overfitting\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # Model checkpoint to save best model\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/best_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # Learning rate reduction on plateau\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=1,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # CSV logger\n",
    "    tf.keras.callbacks.CSVLogger(\n",
    "        'training_log.csv',\n",
    "        separator=',',\n",
    "        append=False\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        save_freq='epoch',\n",
    "        verbose=1\n",
    ")\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "element = None\n",
    "for input, output in train_dataset.take(1):\n",
    "  element = input\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "nqpEpl3WeLRW"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = CustomModel(\n",
    "        lstm_size=300,\n",
    "    )\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(0.001),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.Accuracy(name='accuracy'),\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.F1Score(name='f1')\n",
    "    ]\n",
    "\n",
    ")\n",
    "model(element)\n",
    "model.load_weights('models/best_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qwll09q0W3Oa",
    "outputId": "249c4cc3-530b-41d2-8c27-92ac9525d4eb"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=None,\n",
    "    validation_steps=None,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,  # 0: silent, 1: progress bar, 2: one line per epoch\n",
    "    shuffle=True,  # Shuffle training data\n",
    "    max_queue_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = reader.get_test_dataset(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict(test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pred_data/final_states.npy', outputs['final_states'] )\n",
    "np.save('pred_data/final_predictions.npy', outputs['final_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0WRXgTg7Tyu"
   },
   "outputs": [],
   "source": [
    "model.save_weights(\n",
    "            f'models/epoch_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DAh8Oma7XGu"
   },
   "outputs": [],
   "source": [
    "eval_results = model.evaluate(val_dataset, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGVb4PvP745N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "RecSys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
