{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zcD--o4TxkFf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#from layers import LSTMLayer, WaveNet, TimeDistributedDense, TemporalConvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "56CBbEkbxkFg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mp7AHBpYxkFg"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56Maw5w5xn6Q",
    "outputId": "fd53edb3-7828-4ba7-c36f-960f0edae3ad"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yn81WGF2xqZ3",
    "outputId": "92746340-2529-4143-e621-af79916423b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.zip\n"
     ]
    }
   ],
   "source": [
    "!ls -a /content/drive/MyDrive/recsys_data/rnn_product_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doF07f2rxsGM"
   },
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bo_71tbZxt87",
    "outputId": "ade801c6-bf99-48c2-ac2a-3ab034f34331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/drive/MyDrive/recsys_data/rnn_product_data/data.zip\n",
      "   creating: data/data/\n",
      "  inflating: data/__MACOSX/._data    \n",
      "  inflating: data/data/history_length.npy  \n",
      "  inflating: data/__MACOSX/data/._history_length.npy  \n",
      "  inflating: data/data/product_id.npy  \n",
      "  inflating: data/__MACOSX/data/._product_id.npy  \n",
      "  inflating: data/data/index_in_order_history.npy  \n",
      "  inflating: data/__MACOSX/data/._index_in_order_history.npy  \n",
      "  inflating: data/data/is_ordered_history.npy  \n",
      "  inflating: data/__MACOSX/data/._is_ordered_history.npy  \n",
      "  inflating: data/data/order_hour_history.npy  \n",
      "  inflating: data/__MACOSX/data/._order_hour_history.npy  \n",
      "  inflating: data/data/order_size_history.npy  \n",
      "  inflating: data/__MACOSX/data/._order_size_history.npy  \n",
      "  inflating: data/data/days_since_prior_order_history.npy  \n",
      "  inflating: data/__MACOSX/data/._days_since_prior_order_history.npy  \n",
      "  inflating: data/data/order_dow_history.npy  \n",
      "  inflating: data/__MACOSX/data/._order_dow_history.npy  \n",
      "  inflating: data/data/product_name.npy  \n",
      "  inflating: data/__MACOSX/data/._product_name.npy  \n",
      "  inflating: data/data/user_id.npy   \n",
      "  inflating: data/__MACOSX/data/._user_id.npy  \n",
      "  inflating: data/data/reorder_size_history.npy  \n",
      "  inflating: data/__MACOSX/data/._reorder_size_history.npy  \n",
      "  inflating: data/data/department_id.npy  \n",
      "  inflating: data/__MACOSX/data/._department_id.npy  \n",
      "  inflating: data/data/label.npy     \n",
      "  inflating: data/__MACOSX/data/._label.npy  \n",
      "  inflating: data/data/eval_set.npy  \n",
      "  inflating: data/__MACOSX/data/._eval_set.npy  \n",
      "  inflating: data/data/order_number_history.npy  \n",
      "  inflating: data/__MACOSX/data/._order_number_history.npy  \n",
      "  inflating: data/data/aisle_id.npy  \n",
      "  inflating: data/__MACOSX/data/._aisle_id.npy  \n",
      "  inflating: data/data/product_name_length.npy  \n",
      "  inflating: data/__MACOSX/data/._product_name_length.npy  \n"
     ]
    }
   ],
   "source": [
    "!unzip /content/drive/MyDrive/recsys_data/rnn_product_data/data.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPIsccelOnbk",
    "outputId": "4b33a2b3-1ef2-4358-bb5c-41ecad51b8b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on a TPU w/8 cores\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "  print(f'Running on a TPU w/{tpu.num_accelerators()[\"TPU\"]} cores')\n",
    "except ValueError:\n",
    "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9cs9HK1wTaMj"
   },
   "outputs": [],
   "source": [
    "class TFDataReader2:\n",
    "    def __init__(self, data_dir):\n",
    "        # Define feature columns and label columns\n",
    "        self.feature_cols = [\n",
    "            'user_id',\n",
    "            'department_id',\n",
    "            'is_ordered_history',\n",
    "            'index_in_order_history',\n",
    "            'order_dow_history',\n",
    "            'order_hour_history',\n",
    "            'days_since_prior_order_history',\n",
    "            'order_size_history',\n",
    "            'order_number_history',\n",
    "            'num_products_from_department_history',\n",
    "            'history_length',\n",
    "        ]\n",
    "        self.expand_cols = ['user_id', 'product_id', 'aisle_id', 'department_id',\n",
    "            'history_length', 'product_name_length','label']\n",
    "\n",
    "        # Load all numpy arrays\n",
    "        self.data = {}\n",
    "        for col in self.feature_cols:\n",
    "            self.data[col] = np.load(os.path.join(data_dir, f'{col}.npy'), mmap_mode='r')\n",
    "            #if col in self.expand_cols:\n",
    "                #self.data[col] = self.data[col].reshape(-1,1)\n",
    "        #rint(self.data.keys())\n",
    "        # Create train/val split\n",
    "        total_size = len(next(iter(self.data.values())))\n",
    "        remainder = total_size % 512\n",
    "        for col in self.feature_cols:\n",
    "            self.data[col] = np.concatenate([self.data[col], np.zeros((512-remainder, *self.data[col].shape[1:]), dtype=self.data[col].dtype)], axis=0)\n",
    "        train_size = int(0.9 * total_size)\n",
    "\n",
    "        self.train_indices = np.arange(train_size)\n",
    "        self.val_indices = np.arange(train_size, total_size)\n",
    "        self.all_indices = np.arange(total_size + 512 - remainder)\n",
    "\n",
    "    def _process_features(self, original_features, is_test):\n",
    "\n",
    "        # Create new features dictionary with augmented features\n",
    "        features = {\n",
    "            # Copy original features\n",
    "            **original_features,\n",
    "\n",
    "            # Add augmented features\n",
    "            'order_dow_history': tf.roll(original_features['order_dow_history'], -1, axis=0),\n",
    "            'order_hour_history': tf.roll(original_features['order_hour_history'], -1, axis=0),\n",
    "            'days_since_prior_order_history': tf.roll(original_features['days_since_prior_order_history'], -1, axis=0),\n",
    "            'order_number_history': tf.roll(original_features['order_number_history'], -1, axis=0),\n",
    "        }\n",
    "        # Adjust history length for non-test data\n",
    "        if not is_test:\n",
    "            features['history_length'] = original_features['history_length'] - 1\n",
    "        else:\n",
    "            features['history_length'] = original_features['history_length']\n",
    "        return features, {'next_is_ordered': tf.cast(tf.roll(original_features['is_ordered_history'], -1, axis=0), dtype=tf.float32)}\n",
    "\n",
    "    def _create_dataset(self, indices, shuffle=True, is_test=False):\n",
    "        # Create feature datasets\n",
    "        features_dict = {col: tf.cast(self.data[col][indices], tf.int32) for col in self.feature_cols}\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(features_dict)\n",
    "        # Apply processing before batching\n",
    "        dataset = dataset.map(\n",
    "            lambda x: self._process_features(x, is_test),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=10000)\n",
    "\n",
    "        # Enable prefetching\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def get_train_dataset(self, batch_size):\n",
    "        dataset = self._create_dataset(self.train_indices, shuffle=True)\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        #for element in dataset.take(1):\n",
    "          #print(element[0])\n",
    "        # Process features after batching\n",
    "        #dataset = dataset.map(lambda x: self._process_features((x), is_test=False))\n",
    "        return dataset\n",
    "\n",
    "    def get_val_dataset(self, batch_size):\n",
    "        dataset = self._create_dataset(self.val_indices, shuffle=True)\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        #dataset = dataset.map(lambda x: self._process_features((x), is_test=False))\n",
    "        return dataset\n",
    "\n",
    "    def get_test_dataset(self, batch_size):\n",
    "        dataset = self._create_dataset(self.all_indices, shuffle=False, is_test=True)\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=False)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        #dataset = dataset.map(lambda x: self._process_features((x), is_test=True))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "S7dsk2hHxkFi"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sequence_log_loss(y, y_hat, sequence_lengths, max_sequence_length=100, eps=1e-7):\n",
    "    # Ensure y is float32 for calculations\n",
    "    #y = tf.cast(y, tf.float32)\n",
    "    #print('casted', y.shape)\n",
    "    # Clip predictions to avoid NaNs in log calculations\n",
    "    y_hat = tf.clip_by_value(y_hat, eps, 1.0 - eps)\n",
    "    #print('clipped', y_hat.shape)\n",
    "    # Compute log losses\n",
    "    log_losses = y * tf.math.log(y_hat) + (1.0 - y) * tf.math.log(1.0 - y_hat)\n",
    "    #print(\"logged\", log_losses.shape)\n",
    "    # Create a sequence mask based on sequence lengths\n",
    "    sequence_mask = tf.cast(tf.sequence_mask(sequence_lengths, maxlen=max_sequence_length), tf.float32)\n",
    "    #print(\"mask created\", sequence_mask.shape)\n",
    "    # Apply the sequence mask to the log losses\n",
    "    masked_log_losses = log_losses *sequence_mask\n",
    "    #print(\"masked\", masked_log_losses.shape)\n",
    "    # Compute the average log loss\n",
    "    avg_log_loss = -tf.reduce_sum(masked_log_losses) / tf.cast(tf.reduce_sum(sequence_lengths), tf.float32)\n",
    "    #print(\"reduced\")\n",
    "    return avg_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UwpkzUEE8xtZ"
   },
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(self, lstm_size, **kwargs):\n",
    "        super(CustomModel, self).__init__(**kwargs)\n",
    "        self.lstm_size = lstm_size\n",
    "        self.lstm_layer = tf.keras.layers.GRU(lstm_size, return_sequences=True, dropout=0.0)\n",
    "        self.dense1 = tf.keras.layers.Dense(50, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        self.user_embeddings = tf.keras.layers.Embedding(207000, lstm_size, name='user_embeddings')\n",
    "        self.product_name_dense = tf.keras.layers.Dense(100, activation='relu')\n",
    "    def call(self,inputs):\n",
    "        user_id = inputs['user_id']\n",
    "        department_id = inputs['department_id']\n",
    "        self.history_length = inputs['history_length']\n",
    "\n",
    "        is_ordered_history = inputs['is_ordered_history']\n",
    "        index_in_order_history = inputs['index_in_order_history']\n",
    "        order_dow_history = inputs['order_dow_history']\n",
    "        order_hour_history = inputs['order_hour_history']\n",
    "        days_since_prior_order_history = inputs['days_since_prior_order_history']\n",
    "        order_size_history = inputs['order_size_history']\n",
    "        order_number_history = inputs['order_number_history']\n",
    "        num_products_from_department_history = inputs['num_products_from_department_history']\n",
    "\n",
    "        \n",
    "\n",
    "        #print(product_embeddings.shape, aisle_embeddings.shape, department_embeddings.shape, is_none_float.shape, product_names.shape)\n",
    "        x_department = tf.one_hot(department_id, 30)\n",
    "        x_department = tf.tile(tf.expand_dims(x_department, 1), (1, 100, 1))\n",
    "\n",
    "        # User data\n",
    "        user_embeddings = self.user_embeddings(user_id)\n",
    "        x_user = tf.tile(tf.expand_dims(user_embeddings, 1), (1, 100, 1))\n",
    "\n",
    "        # Sequence data\n",
    "        is_ordered_history_onehot = tf.one_hot(is_ordered_history, 2)\n",
    "        index_in_order_history_onehot = tf.one_hot(index_in_order_history, 20)\n",
    "        order_dow_history_onehot = tf.one_hot(order_dow_history, 8)\n",
    "        order_hour_history_onehot = tf.one_hot(order_hour_history, 25)\n",
    "        days_since_prior_order_history_onehot = tf.one_hot(days_since_prior_order_history, 31)\n",
    "        order_size_history_onehot = tf.one_hot(order_size_history, 60)\n",
    "        order_number_history_onehot = tf.one_hot(order_number_history, 101)\n",
    "        num_products_from_department_history_onehot = tf.one_hot(num_products_from_department_history, 50)\n",
    "\n",
    "\n",
    "        index_in_order_history_scalar = tf.expand_dims(tf.cast(index_in_order_history, tf.float32) / 20.0, 2)\n",
    "        order_dow_history_scalar = tf.expand_dims(tf.cast(order_dow_history, tf.float32) / 8.0, 2)\n",
    "        order_hour_history_scalar = tf.expand_dims(tf.cast(order_hour_history, tf.float32) / 25.0, 2)\n",
    "        days_since_prior_order_history_scalar = tf.expand_dims(tf.cast(days_since_prior_order_history, tf.float32) / 31.0, 2)\n",
    "        order_size_history_scalar = tf.expand_dims(tf.cast(order_size_history, tf.float32) / 60.0, 2)\n",
    "        order_number_history_scalar = tf.expand_dims(tf.cast(order_number_history, tf.float32) / 100.0, 2)\n",
    "        num_products_from_department_history_scalar = tf.expand_dims(tf.cast(num_products_from_department_history, tf.float32) / 50.0, 2)\n",
    "\n",
    "\n",
    "        x_history = tf.concat([\n",
    "            is_ordered_history_onehot,\n",
    "            index_in_order_history_onehot,\n",
    "            order_dow_history_onehot,\n",
    "            order_hour_history_onehot,\n",
    "            days_since_prior_order_history_onehot,\n",
    "            order_size_history_onehot,\n",
    "            order_number_history_onehot,\n",
    "            num_products_from_department_history_onehot,\n",
    "            index_in_order_history_scalar,\n",
    "            order_dow_history_scalar,\n",
    "            order_hour_history_scalar,\n",
    "            days_since_prior_order_history_scalar,\n",
    "            order_size_history_scalar,\n",
    "            order_number_history_scalar,\n",
    "            num_products_from_department_history_scalar,\n",
    "        ], axis=2)\n",
    "\n",
    "        outputs = tf.concat([x_history, x_department, x_user], axis=2)\n",
    "        h = self.lstm_layer(outputs)\n",
    "        h = tf.concat([h,outputs], axis=-1)\n",
    "        h = tf.keras.layers.TimeDistributed(self.dense1, name='hidden_states')(h)\n",
    "        y_hat = tf.keras.layers.TimeDistributed(self.dense2)(h)\n",
    "        y_hat = tf.squeeze(y_hat, axis=-1)\n",
    "        \n",
    "        final_temporal_idx = tf.stack([tf.range(tf.shape(self.history_length)[0]), self.history_length - 1], axis=1)\n",
    "        final_states = tf.gather_nd(h, final_temporal_idx)\n",
    "        final_predictions = tf.gather_nd(y_hat, final_temporal_idx)\n",
    "        # final_states = tf.keras.layers.Lambda(lambda x: x, name='final_states')(final_states)\n",
    "        # final_predictions = tf.keras.layers.Lambda(lambda x: x, name='final_predictions')(final_predictions)\n",
    "        return {'next_is_ordered': y_hat, 'final_states': final_states, 'final_predictions': final_predictions}\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        history_length = x['history_length']\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            # Pass both prediction and history_length to loss\n",
    "            loss = sequence_log_loss(y['next_is_ordered'], y_pred['next_is_ordered'], history_length, 100)\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(y['next_is_ordered'], y_pred['next_is_ordered'])\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results['loss'] = loss\n",
    "        return results\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        history_length = x['history_length']\n",
    "\n",
    "        y_pred = self(x, training=False)\n",
    "        # Pass both prediction and history_length to loss\n",
    "        loss = sequence_log_loss(y['next_is_ordered'], y_pred['next_is_ordered'], history_length, 100)\n",
    "\n",
    "        self.compiled_metrics.update_state(y['next_is_ordered'], y_pred['next_is_ordered'])\n",
    "\n",
    "        # Return metrics and val_loss\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results['loss'] = loss\n",
    "        return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLw5C3QeRRkV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "I_4Z20f7cZ6u"
   },
   "outputs": [],
   "source": [
    "reader = TFDataReader2('data')\n",
    "train_dataset = reader.get_train_dataset(256)\n",
    "val_dataset = reader.get_val_dataset(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YVCMF4NJ_yF9"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/checkpoints/cp-{epoch:04d}.ckpt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0kauFm0OednL"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    # Early stopping to prevent overfitting\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # Model checkpoint to save best model\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/best_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # Learning rate reduction on plateau\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=1,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # CSV logger\n",
    "    tf.keras.callbacks.CSVLogger(\n",
    "        'training_log.csv',\n",
    "        separator=',',\n",
    "        append=False\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        save_freq='epoch',\n",
    "        verbose=1\n",
    ")\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nqpEpl3WeLRW"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = CustomModel(\n",
    "        lstm_size=300,\n",
    "    )\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(0.001),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.Accuracy(name='accuracy'),\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.F1Score(name='f1')\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qwll09q0W3Oa",
    "outputId": "249c4cc3-530b-41d2-8c27-92ac9525d4eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 09:21:46.953047: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-12-20 09:21:47.909693: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-12-20 09:21:50.014930: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2487/7849 [========>.....................] - ETA: 34:22 - accuracy: 0.0000e+00 - auc: 0.9037 - precision: 0.5934 - recall: 0.5363 - f1: 0.0333 - loss: 0.4966"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 0: silent, 1: progress bar, 2: one line per epoch\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Shuffle training data\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RecSys/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RecSys/lib/python3.9/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RecSys/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RecSys/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RecSys/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:864\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    862\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RecSys/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RecSys/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RecSys/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RecSys/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RecSys/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=None,\n",
    "    validation_steps=None,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,  # 0: silent, 1: progress bar, 2: one line per epoch\n",
    "    shuffle=True,  # Shuffle training data\n",
    "    max_queue_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0WRXgTg7Tyu"
   },
   "outputs": [],
   "source": [
    "model.save_weights(\n",
    "            f'models/epoch_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DAh8Oma7XGu"
   },
   "outputs": [],
   "source": [
    "eval_results = model.evaluate(val_dataset, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGVb4PvP745N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "RecSys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
