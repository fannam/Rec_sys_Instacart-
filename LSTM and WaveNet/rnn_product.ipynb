{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zcD--o4TxkFf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "#from layers import LSTMLayer, WaveNet, TimeDistributedDense, TemporalConvolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "56CBbEkbxkFg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mp7AHBpYxkFg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56Maw5w5xn6Q",
        "outputId": "fd53edb3-7828-4ba7-c36f-960f0edae3ad"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn81WGF2xqZ3",
        "outputId": "92746340-2529-4143-e621-af79916423b5"
      },
      "outputs": [],
      "source": [
        "!ls -a /content/drive/MyDrive/recsys_data/rnn_product_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "doF07f2rxsGM"
      },
      "outputs": [],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo_71tbZxt87",
        "outputId": "ade801c6-bf99-48c2-ac2a-3ab034f34331"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/recsys_data/rnn_product_data/data.zip -d data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPIsccelOnbk",
        "outputId": "4b33a2b3-1ef2-4358-bb5c-41ecad51b8b0"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print(f'Running on a TPU w/{tpu.num_accelerators()[\"TPU\"]} cores')\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XZBKfAj9xkFh"
      },
      "outputs": [],
      "source": [
        "class TFDataReader:\n",
        "    def __init__(self, data_dir):\n",
        "        # Define feature columns and label columns\n",
        "        self.feature_cols = [\n",
        "            'user_id', 'product_id', 'aisle_id', 'department_id',\n",
        "            'is_ordered_history', 'index_in_order_history',\n",
        "            'order_dow_history', 'order_hour_history',\n",
        "            'days_since_prior_order_history', 'order_size_history',\n",
        "            'reorder_size_history', 'order_number_history',\n",
        "            'history_length', 'product_name', 'product_name_length',\n",
        "        ]\n",
        "        self.expand_cols = ['user_id', 'product_id', 'aisle_id', 'department_id',\n",
        "            'history_length', 'product_name_length','label']\n",
        "        self.label_cols = ['label']\n",
        "\n",
        "        # Load all numpy arrays\n",
        "        self.data = {}\n",
        "        for col in self.feature_cols + self.label_cols:\n",
        "            self.data[col] = np.load(os.path.join(data_dir, f'{col}.npy'), mmap_mode='r')[:10000]\n",
        "            #if col in self.expand_cols:\n",
        "            #    self.data[col] = self.data[col].reshape(-1,1)\n",
        "        #rint(self.data.keys())\n",
        "        # Create train/val split\n",
        "        total_size = len(next(iter(self.data.values())))\n",
        "        train_size = int(0.9 * total_size)\n",
        "\n",
        "        self.train_indices = np.arange(train_size)\n",
        "        self.val_indices = np.arange(train_size, total_size)\n",
        "        self.all_indices = np.arange(total_size)\n",
        "\n",
        "    def _process_features(self, original_features, is_test):\n",
        "\n",
        "        # Create new features dictionary with augmented features\n",
        "        features = {\n",
        "            # Copy original features\n",
        "            **original_features,\n",
        "\n",
        "            # Add augmented features\n",
        "            'order_dow_history': tf.roll(original_features['order_dow_history'], -1, axis=1),\n",
        "            'order_hour_history': tf.roll(original_features['order_hour_history'], -1, axis=1),\n",
        "            'days_since_prior_order_history': tf.roll(original_features['days_since_prior_order_history'], -1, axis=1),\n",
        "            'order_number_history': tf.roll(original_features['order_number_history'], -1, axis=1),\n",
        "            'is_none': tf.cast(tf.equal(original_features['product_id'],\n",
        "                                    tf.constant(0, dtype=tf.int32)), tf.int32)\n",
        "        }\n",
        "        # Adjust history length for non-test data\n",
        "        if not is_test:\n",
        "            features['history_length'] = original_features['history_length'] - 1\n",
        "        else:\n",
        "            features['history_length'] = original_features['history_length']\n",
        "        for key in features.keys():\n",
        "            print(key, features[key].shape)\n",
        "        return features, {'next_is_ordered': tf.cast(tf.roll(original_features['is_ordered_history'], -1, axis=1), dtype=tf.float32)}\n",
        "\n",
        "    def _create_dataset(self, indices, shuffle=True, is_test=False):\n",
        "        # Create feature datasets\n",
        "        feature_datasets = {\n",
        "            col: tf.data.Dataset.from_tensor_slices(tf.cast(self.data[col][indices], tf.int32))\n",
        "            for col in self.feature_cols\n",
        "        }\n",
        "        #print(\"Feature datasets keys:\", feature_datasets.keys())\n",
        "\n",
        "        # Create label datasets\n",
        "        # label_datasets = {\n",
        "        #     col: tf.data.Dataset.from_tensor_slices(tf.cast(self.data[col][indices], tf.float32))\n",
        "        #     for col in self.label_cols\n",
        "        # }\n",
        "\n",
        "        # # Combine features into a single dictionary dataset\n",
        "        features_dataset = tf.data.Dataset.zip(feature_datasets)\n",
        "        # #for element in features_dataset.take(1):\n",
        "        #   #print(\"After zip - element structure:\", element)\n",
        "\n",
        "        # # Combine labels into a single dictionary dataset\n",
        "        # labels_dataset = tf.data.Dataset.zip(label_datasets)\n",
        "\n",
        "        # # Combine features and labels\n",
        "        # dataset = tf.data.Dataset.zip((features_dataset, labels_dataset))\n",
        "\n",
        "        if shuffle:\n",
        "            features_dataset = features_dataset.shuffle(buffer_size=10000)\n",
        "\n",
        "        return features_dataset\n",
        "\n",
        "    def get_train_dataset(self, batch_size):\n",
        "        dataset = self._create_dataset(self.train_indices, shuffle=True)\n",
        "        dataset = dataset.batch(batch_size)\n",
        "        #for element in dataset.take(1):\n",
        "          #print(element[0])\n",
        "        # Process features after batching\n",
        "        dataset = dataset.map(lambda x: self._process_features((x), is_test=False))\n",
        "        return dataset\n",
        "\n",
        "    def get_val_dataset(self, batch_size):\n",
        "        dataset = self._create_dataset(self.val_indices, shuffle=True)\n",
        "        dataset = dataset.batch(batch_size)\n",
        "        dataset = dataset.map(lambda x: self._process_features((x), is_test=False))\n",
        "        return dataset\n",
        "\n",
        "    def get_test_dataset(self, batch_size):\n",
        "        dataset = self._create_dataset(self.all_indices, shuffle=False, is_test=True)\n",
        "        dataset = dataset.batch(batch_size, drop_remainder=False)\n",
        "        dataset = dataset.map(lambda x: self._process_features((x), is_test=True))\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "9cs9HK1wTaMj"
      },
      "outputs": [],
      "source": [
        "class TFDataReader2:\n",
        "    def __init__(self, data_dir):\n",
        "        # Define feature columns and label columns\n",
        "        self.feature_cols = [\n",
        "            'user_id', 'product_id', 'aisle_id', 'department_id',\n",
        "            'is_ordered_history', 'index_in_order_history',\n",
        "            'order_dow_history', 'order_hour_history',\n",
        "            'days_since_prior_order_history', 'order_size_history',\n",
        "            'reorder_size_history', 'order_number_history',\n",
        "            'history_length', 'product_name', 'product_name_length',\n",
        "        ]\n",
        "        self.expand_cols = ['user_id', 'product_id', 'aisle_id', 'department_id',\n",
        "            'history_length', 'product_name_length','label']\n",
        "        self.label_cols = ['label']\n",
        "\n",
        "        # Load all numpy arrays\n",
        "        self.data = {}\n",
        "        for col in self.feature_cols + self.label_cols:\n",
        "            self.data[col] = np.load(os.path.join(data_dir, f'{col}.npy'), mmap_mode='r')[:10000]\n",
        "            #if col in self.expand_cols:\n",
        "                #self.data[col] = self.data[col].reshape(-1,1)\n",
        "        #rint(self.data.keys())\n",
        "        # Create train/val split\n",
        "        total_size = len(next(iter(self.data.values())))\n",
        "        train_size = int(0.9 * total_size)\n",
        "\n",
        "        self.train_indices = np.arange(train_size)\n",
        "        self.val_indices = np.arange(train_size, total_size)\n",
        "        self.all_indices = np.arange(total_size)\n",
        "\n",
        "    def _process_features(self, original_features, is_test):\n",
        "\n",
        "        # Create new features dictionary with augmented features\n",
        "        batch_size = original_features['user_id'].shape[0]\n",
        "        features = {\n",
        "            # Copy original features\n",
        "            **original_features,\n",
        "\n",
        "            # Add augmented features\n",
        "            'order_dow_history': tf.roll(original_features['order_dow_history'], -1, axis=0),\n",
        "            'order_hour_history': tf.roll(original_features['order_hour_history'], -1, axis=0),\n",
        "            'days_since_prior_order_history': tf.roll(original_features['days_since_prior_order_history'], -1, axis=0),\n",
        "            'order_number_history': tf.roll(original_features['order_number_history'], -1, axis=0),\n",
        "            'is_none': tf.cast(tf.equal(original_features['product_id'],\n",
        "                                    tf.constant(0, dtype=tf.int32)), tf.int32)\n",
        "        }\n",
        "        # Adjust history length for non-test data\n",
        "        if not is_test:\n",
        "            features['history_length'] = original_features['history_length'] - 1\n",
        "        else:\n",
        "            features['history_length'] = original_features['history_length']\n",
        "        return features, {'in_next_order': tf.cast(features['is_ordered_history'][np.arange(batch_size), features['history_length'] - 1], dtype=tf.float32)}\n",
        "\n",
        "    def _create_dataset(self, indices, shuffle=True, is_test=False):\n",
        "        # Create feature datasets\n",
        "        features_dict = {col: tf.cast(self.data[col][indices], tf.int32) for col in self.feature_cols}\n",
        "\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(features_dict)\n",
        "        # Apply processing before batching\n",
        "        dataset = dataset.map(\n",
        "            lambda x: self._process_features(x, is_test),\n",
        "            num_parallel_calls=tf.data.AUTOTUNE\n",
        "        )\n",
        "        if shuffle:\n",
        "            dataset = dataset.shuffle(buffer_size=10000)\n",
        "\n",
        "        # Enable prefetching\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def get_train_dataset(self, batch_size):\n",
        "        dataset = self._create_dataset(self.train_indices, shuffle=True)\n",
        "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "        #for element in dataset.take(1):\n",
        "          #print(element[0])\n",
        "        # Process features after batching\n",
        "        #dataset = dataset.map(lambda x: self._process_features((x), is_test=False))\n",
        "        return dataset\n",
        "\n",
        "    def get_val_dataset(self, batch_size):\n",
        "        dataset = self._create_dataset(self.val_indices, shuffle=True)\n",
        "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "        #dataset = dataset.map(lambda x: self._process_features((x), is_test=False))\n",
        "        return dataset\n",
        "\n",
        "    def get_test_dataset(self, batch_size):\n",
        "        dataset = self._create_dataset(self.all_indices, shuffle=False, is_test=True)\n",
        "        dataset = dataset.batch(batch_size, drop_remainder=False)\n",
        "        #dataset = dataset.map(lambda x: self._process_features((x), is_test=True))\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RXI4WcedxkFh"
      },
      "outputs": [],
      "source": [
        "class WaveNetLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, dilations, filter_widths, skip_channels, residual_channels, **kwargs):\n",
        "        super(WaveNetLayer, self).__init__(**kwargs)\n",
        "        self.dilations = dilations\n",
        "        self.filter_widths = filter_widths\n",
        "        self.skip_channels = skip_channels\n",
        "        self.residual_channels = residual_channels\n",
        "\n",
        "        # Initial projection\n",
        "        self.input_proj = tf.keras.layers.Conv1D(\n",
        "            filters=residual_channels,\n",
        "            kernel_size=1,\n",
        "            activation='tanh',\n",
        "            name='x-proj'\n",
        "        )\n",
        "\n",
        "        # Dilated convolution layers\n",
        "        self.dilated_conv_layers = []\n",
        "        self.output_projs = []\n",
        "\n",
        "        for i, (dilation, filter_width) in enumerate(zip(dilations, filter_widths)):\n",
        "            self.dilated_conv_layers.append(\n",
        "                tf.keras.layers.Conv1D(\n",
        "                    filters=2*residual_channels,  # Double for gate and filter\n",
        "                    kernel_size=filter_width,\n",
        "                    padding='causal',\n",
        "                    dilation_rate=dilation,\n",
        "                    name=f'cnn-{i}'\n",
        "                )\n",
        "            )\n",
        "\n",
        "            self.output_projs.append(\n",
        "                tf.keras.layers.Conv1D(\n",
        "                    filters=skip_channels + residual_channels,\n",
        "                    kernel_size=1,\n",
        "                    name=f'cnn-{i}-proj'\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Initial projection\n",
        "        x = self.input_proj(inputs)\n",
        "\n",
        "        skip_outputs = []\n",
        "        inputs_plus_residuals = x\n",
        "\n",
        "        # Process through dilated convolutions\n",
        "        for dilated_conv, output_proj in zip(self.dilated_conv_layers, self.output_projs):\n",
        "            # Dilated convolution\n",
        "            dilated_out = dilated_conv(inputs_plus_residuals)\n",
        "\n",
        "            # Split and apply gating\n",
        "            conv_filter, conv_gate = tf.split(dilated_out, 2, axis=-1)\n",
        "            dilated_out = tf.nn.tanh(conv_filter) * tf.nn.sigmoid(conv_gate)\n",
        "\n",
        "            # Project to skip and residual\n",
        "            outputs = output_proj(dilated_out)\n",
        "            skips, residuals = tf.split(\n",
        "                outputs,\n",
        "                [self.skip_channels, self.residual_channels],\n",
        "                axis=-1\n",
        "            )\n",
        "\n",
        "            # Add residual connection\n",
        "            inputs_plus_residuals += residuals\n",
        "            skip_outputs.append(skips)\n",
        "\n",
        "        # Combine skip connections\n",
        "        skip_outputs = tf.concat(skip_outputs, axis=-1)\n",
        "        return tf.nn.relu(skip_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "S7dsk2hHxkFi"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def sequence_log_loss(y, y_hat, sequence_lengths, max_sequence_length=100, eps=1e-7):\n",
        "    # Ensure y is float32 for calculations\n",
        "    #y = tf.cast(y, tf.float32)\n",
        "    #print('casted', y.shape)\n",
        "    # Clip predictions to avoid NaNs in log calculations\n",
        "    y_hat = tf.clip_by_value(y_hat, eps, 1.0 - eps)\n",
        "    #print('clipped', y_hat.shape)\n",
        "    # Compute log losses\n",
        "    log_losses = y * tf.math.log(y_hat) + (1.0 - y) * tf.math.log(1.0 - y_hat)\n",
        "    #print(\"logged\", log_losses.shape)\n",
        "    # Create a sequence mask based on sequence lengths\n",
        "    sequence_mask = tf.cast(tf.sequence_mask(sequence_lengths, maxlen=max_sequence_length), tf.float32)\n",
        "    #print(\"mask created\", sequence_mask.shape)\n",
        "    # Apply the sequence mask to the log losses\n",
        "    masked_log_losses = log_losses *sequence_mask\n",
        "    #print(\"masked\", masked_log_losses.shape)\n",
        "    # Compute the average log loss\n",
        "    avg_log_loss = -tf.reduce_sum(masked_log_losses) / tf.cast(tf.reduce_sum(sequence_lengths), tf.float32)\n",
        "    #print(\"reduced\")\n",
        "    return avg_log_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UwpkzUEE8xtZ"
      },
      "outputs": [],
      "source": [
        "class CustomModel(tf.keras.Model):\n",
        "    def __init__(self, lstm_size, dilations, filter_widths, skip_channels, residual_channels, **kwargs):\n",
        "        super(CustomModel, self).__init__(**kwargs)\n",
        "        self.lstm_size = lstm_size\n",
        "        self.lstm_layer = tf.keras.layers.GRU(lstm_size, return_sequences=True, dropout=0.0)\n",
        "        self.wavenet = WaveNetLayer(dilations, filter_widths, skip_channels, residual_channels)\n",
        "        self.dense1 = tf.keras.layers.Dense(50, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        self.product_embeddings = tf.keras.layers.Embedding(50000, lstm_size, name='product_embeddings')\n",
        "        self.aisle_embeddings = tf.keras.layers.Embedding(250, 50, name='aisle_embeddings')\n",
        "        self.department_embeddings = tf.keras.layers.Embedding(50, 10, name='department_embeddings')\n",
        "        self.user_embeddings = tf.keras.layers.Embedding(207000, lstm_size, name='user_embeddings')\n",
        "        self.product_name_dense = tf.keras.layers.Dense(100, activation='relu')\n",
        "        self.bce_loss_func = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    def call(self,inputs):\n",
        "        user_id = inputs['user_id']\n",
        "        product_id = inputs['product_id']\n",
        "        aisle_id = inputs['aisle_id']\n",
        "        department_id = inputs['department_id']\n",
        "        is_none = inputs['is_none']\n",
        "        self.history_length = inputs['history_length']\n",
        "\n",
        "        is_ordered_history = inputs['is_ordered_history']\n",
        "        index_in_order_history = inputs['index_in_order_history']\n",
        "        order_dow_history = inputs['order_dow_history']\n",
        "        order_hour_history = inputs['order_hour_history']\n",
        "        days_since_prior_order_history = inputs['days_since_prior_order_history']\n",
        "        order_size_history = inputs['order_size_history']\n",
        "        reorder_size_history = inputs['reorder_size_history']\n",
        "        order_number_history = inputs['order_number_history']\n",
        "        product_name = inputs['product_name']\n",
        "\n",
        "        product_names = tf.one_hot(product_name, 2532)\n",
        "        #print(product_names.shape)\n",
        "        product_names = tf.reduce_max(product_names, axis=1)\n",
        "        #print(product_names.shape)\n",
        "        product_names = self.product_name_dense(product_names)\n",
        "        #print(product_names.shape)\n",
        "\n",
        "        is_none_float = tf.cast(tf.expand_dims(is_none, 1), tf.float32)\n",
        "\n",
        "        product_embeddings = self.product_embeddings(product_id)\n",
        "        aisle_embeddings = self.aisle_embeddings(aisle_id)\n",
        "        department_embeddings = self.department_embeddings(department_id)\n",
        "\n",
        "        #print(product_embeddings.shape, aisle_embeddings.shape, department_embeddings.shape, is_none_float.shape, product_names.shape)\n",
        "        x_product = tf.concat([\n",
        "            product_embeddings,\n",
        "            aisle_embeddings,\n",
        "            department_embeddings,\n",
        "            is_none_float,\n",
        "            product_names\n",
        "        ], axis=1)\n",
        "        x_product = tf.tile(tf.expand_dims(x_product, 1), (1, 100, 1))\n",
        "\n",
        "        # User data\n",
        "        user_embeddings = self.user_embeddings(user_id)\n",
        "        x_user = tf.tile(tf.expand_dims(user_embeddings, 1), (1, 100, 1))\n",
        "\n",
        "        # Sequence data\n",
        "        is_ordered_history_onehot = tf.one_hot(is_ordered_history, 2)\n",
        "        index_in_order_history_onehot = tf.one_hot(index_in_order_history, 20)\n",
        "        order_dow_history_onehot = tf.one_hot(order_dow_history, 8)\n",
        "        order_hour_history_onehot = tf.one_hot(order_hour_history, 25)\n",
        "        days_since_prior_order_history_onehot = tf.one_hot(days_since_prior_order_history, 31)\n",
        "        order_size_history_onehot = tf.one_hot(order_size_history, 60)\n",
        "        reorder_size_history_onehot = tf.one_hot(reorder_size_history, 50)\n",
        "        order_number_history_onehot = tf.one_hot(order_number_history, 101)\n",
        "\n",
        "        index_in_order_history_scalar = tf.expand_dims(tf.cast(index_in_order_history, tf.float32) / 20.0, 2)\n",
        "        order_dow_history_scalar = tf.expand_dims(tf.cast(order_dow_history, tf.float32) / 8.0, 2)\n",
        "        order_hour_history_scalar = tf.expand_dims(tf.cast(order_hour_history, tf.float32) / 25.0, 2)\n",
        "        days_since_prior_order_history_scalar = tf.expand_dims(tf.cast(days_since_prior_order_history, tf.float32) / 31.0, 2)\n",
        "        order_size_history_scalar = tf.expand_dims(tf.cast(order_size_history, tf.float32) / 60.0, 2)\n",
        "        reorder_size_history_scalar = tf.expand_dims(tf.cast(reorder_size_history, tf.float32) / 50.0, 2)\n",
        "        order_number_history_scalar = tf.expand_dims(tf.cast(order_number_history, tf.float32) / 100.0, 2)\n",
        "\n",
        "        x_history = tf.concat([\n",
        "            is_ordered_history_onehot,\n",
        "            index_in_order_history_onehot,\n",
        "            order_dow_history_onehot,\n",
        "            order_hour_history_onehot,\n",
        "            days_since_prior_order_history_onehot,\n",
        "            order_size_history_onehot,\n",
        "            reorder_size_history_onehot,\n",
        "            order_number_history_onehot,\n",
        "            index_in_order_history_scalar,\n",
        "            order_dow_history_scalar,\n",
        "            order_hour_history_scalar,\n",
        "            days_since_prior_order_history_scalar,\n",
        "            order_size_history_scalar,\n",
        "            reorder_size_history_scalar,\n",
        "            order_number_history_scalar,\n",
        "        ], axis=2)\n",
        "\n",
        "        outputs = tf.concat([x_history, x_product, x_user], axis=2)\n",
        "        print(outputs.shape)\n",
        "        h = self.lstm_layer(outputs)\n",
        "        print(h.shape)\n",
        "        c = self.wavenet(outputs)\n",
        "        print(c.shape)\n",
        "        h = tf.concat([h,c,outputs], axis=-1)\n",
        "        h = tf.keras.layers.TimeDistributed(self.dense1, name='hidden_states')(h)\n",
        "        y_hat = tf.keras.layers.TimeDistributed(self.dense2)(h)\n",
        "        y_hat = tf.squeeze(y_hat, axis=-1)\n",
        "        final_temporal_idx = tf.stack([tf.range(tf.shape(self.history_length)[0]), self.history_length - 1], axis=1)\n",
        "        final_states = tf.gather_nd(h, final_temporal_idx)\n",
        "        print(final_states.shape)\n",
        "        print(y_hat.shape)\n",
        "        final_predictions = tf.gather_nd(y_hat, final_temporal_idx)\n",
        "        # final_states = tf.keras.layers.Lambda(lambda x: x, name='final_states')(final_states)\n",
        "        # final_predictions = tf.keras.layers.Lambda(lambda x: x, name='final_predictions')(final_predictions)\n",
        "        return {'next_is_ordered': y_hat, 'final_states': final_states, 'in_next_order': final_predictions}\n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        history_length = x['history_length']\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            # Pass both prediction and history_length to loss\n",
        "            loss = sequence_log_loss(y['next_is_ordered'], y_pred['next_is_ordered'], history_length, 100)\n",
        "\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.compiled_metrics.update_state(y['next_is_ordered'], y_pred['next_is_ordered'])\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "    @tf.function\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "        history_length = x['history_length']\n",
        "\n",
        "        y_pred = self(x, training=False)\n",
        "        # Pass both prediction and history_length to loss\n",
        "        loss = sequence_log_loss(y['next_is_ordered'], y_pred['next_is_ordered'], history_length, 100)\n",
        "\n",
        "        trainable_vars = self.trainable_variables\n",
        "        self.compiled_metrics.update_state(y['next_is_ordered'], y_pred['next_is_ordered'])\n",
        "\n",
        "        # Return metrics and val_loss\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results['loss'] = loss\n",
        "        return results\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YLw5C3QeRRkV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "I_4Z20f7cZ6u"
      },
      "outputs": [],
      "source": [
        "reader = TFDataReader2('data')\n",
        "train_dataset = reader.get_train_dataset(128)\n",
        "val_dataset = reader.get_val_dataset(512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "YVCMF4NJ_yF9"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/recsys_data/checkpoints/rnn_products/cp-{epoch:04d}.ckpt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "0kauFm0OednL"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    # Early stopping to prevent overfitting\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # Model checkpoint to save best model\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath='models/best_model.h5',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # Learning rate reduction on plateau\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # CSV logger\n",
        "    tf.keras.callbacks.CSVLogger(\n",
        "        'training_log.csv',\n",
        "        separator=',',\n",
        "        append=False\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=True,\n",
        "        save_freq='epoch',\n",
        "        verbose=1\n",
        ")\n",
        "\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "element = None\n",
        "for el in val_dataset.take(1):\n",
        "    element = el[0]\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqpEpl3WeLRW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model = CustomModel(\n",
        "        lstm_size=300,\n",
        "        dilations=[2**i for i in range(6)],\n",
        "        filter_widths=[2]*6,\n",
        "        skip_channels=64,\n",
        "        residual_channels=128\n",
        "    )\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.Accuracy(name='accuracy'),\n",
        "        tf.keras.metrics.AUC(name='auc'),\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.Recall(name='recall'),\n",
        "        tf.keras.metrics.F1Score(name='f1')\n",
        "    ]\n",
        ")\n",
        "model(element)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwll09q0W3Oa",
        "outputId": "249c4cc3-530b-41d2-8c27-92ac9525d4eb"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=None,\n",
        "    validation_steps=None,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,  # 0: silent, 1: progress bar, 2: one line per epoch\n",
        "    shuffle=True,  # Shuffle training data\n",
        "    max_queue_size=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "n0WRXgTg7Tyu"
      },
      "outputs": [],
      "source": [
        "model.save_weights(\n",
        "            f'models/epoch_1.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DAh8Oma7XGu"
      },
      "outputs": [],
      "source": [
        "eval_results = model.evaluate(val_dataset, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGVb4PvP745N"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "RecSys",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
