{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zcD--o4TxkFf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#from layers import LSTMLayer, WaveNet, TimeDistributedDense, TemporalConvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "56CBbEkbxkFg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mp7AHBpYxkFg"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56Maw5w5xn6Q",
    "outputId": "fd53edb3-7828-4ba7-c36f-960f0edae3ad"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yn81WGF2xqZ3",
    "outputId": "92746340-2529-4143-e621-af79916423b5"
   },
   "outputs": [],
   "source": [
    "!ls -a /content/drive/MyDrive/recsys_data/rnn_product_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doF07f2rxsGM"
   },
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bo_71tbZxt87",
    "outputId": "ade801c6-bf99-48c2-ac2a-3ab034f34331"
   },
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/recsys_data/rnn_product_data/data.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPIsccelOnbk",
    "outputId": "4b33a2b3-1ef2-4358-bb5c-41ecad51b8b0"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "  print(f'Running on a TPU w/{tpu.num_accelerators()[\"TPU\"]} cores')\n",
    "except ValueError:\n",
    "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9cs9HK1wTaMj"
   },
   "outputs": [],
   "source": [
    "class TFDataReader2:\n",
    "    def __init__(self, data_dir):\n",
    "        # Define feature columns and label columns\n",
    "        self.feature_cols = [\n",
    "            'user_id',\n",
    "            'history_length',\n",
    "            'order_size_history',\n",
    "            'reorder_size_history',\n",
    "            'order_number_history',\n",
    "            'order_dow_history',\n",
    "            'order_hour_history',\n",
    "            'days_since_prior_order_history',\n",
    "        ]\n",
    "        self.expand_cols = ['user_id', 'product_id', 'aisle_id', 'department_id',\n",
    "            'history_length', 'product_name_length','label']\n",
    "\n",
    "        # Load all numpy arrays\n",
    "        self.data = {}\n",
    "        for col in self.feature_cols:\n",
    "            self.data[col] = np.load(os.path.join(data_dir, f'{col}.npy'), mmap_mode='r')\n",
    "            #if col in self.expand_cols:\n",
    "                #self.data[col] = self.data[col].reshape(-1,1)\n",
    "        #rint(self.data.keys())\n",
    "        # Create train/val split\n",
    "        total_size = len(next(iter(self.data.values())))\n",
    "        remainder = total_size % 512\n",
    "        for col in self.feature_cols:\n",
    "            self.data[col] = np.concatenate([self.data[col], np.zeros((512-remainder, *self.data[col].shape[1:]), dtype=self.data[col].dtype)], axis=0)\n",
    "        train_size = int(0.9 * total_size)\n",
    "\n",
    "        self.train_indices = np.arange(train_size)\n",
    "        self.val_indices = np.arange(train_size, total_size)\n",
    "        self.all_indices = np.arange(total_size + 512 - remainder)\n",
    "\n",
    "    def _process_features(self, original_features, is_test):\n",
    "\n",
    "        # Create new features dictionary with augmented features\n",
    "        features = {\n",
    "            # Copy original features\n",
    "            **original_features,\n",
    "\n",
    "            # Add augmented features\n",
    "            'order_dow_history': tf.roll(original_features['order_dow_history'], -1, axis=0),\n",
    "            'order_hour_history': tf.roll(original_features['order_hour_history'], -1, axis=0),\n",
    "            'days_since_prior_order_history': tf.roll(original_features['days_since_prior_order_history'], -1, axis=0),\n",
    "            'order_number_history': tf.roll(original_features['order_number_history'], -1, axis=0),\n",
    "            'next_reorder_size': tf.cast(tf.roll(original_features['reorder_size_history'], -1, axis=0), dtype=tf.float32)\n",
    "        }\n",
    "        # Adjust history length for non-test data\n",
    "        if not is_test:\n",
    "            features['history_length'] = original_features['history_length'] - 1\n",
    "        else:\n",
    "            features['history_length'] = original_features['history_length']\n",
    "        return features, {'next_reorder_size': tf.cast(tf.roll(original_features['reorder_size_history'], -1, axis=0), dtype=tf.float32)}\n",
    "\n",
    "    def _create_dataset(self, indices, shuffle=True, is_test=False):\n",
    "        # Create feature datasets\n",
    "        features_dict = {col: tf.cast(self.data[col][indices], tf.int32) for col in self.feature_cols}\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(features_dict)\n",
    "        # Apply processing before batching\n",
    "        dataset = dataset.map(\n",
    "            lambda x: self._process_features(x, is_test),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=10000)\n",
    "\n",
    "        # Enable prefetching\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def get_train_dataset(self, batch_size):\n",
    "        dataset = self._create_dataset(self.train_indices, shuffle=True)\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        #for element in dataset.take(1):\n",
    "          #print(element[0])\n",
    "        # Process features after batching\n",
    "        #dataset = dataset.map(lambda x: self._process_features((x), is_test=False))\n",
    "        return dataset\n",
    "\n",
    "    def get_val_dataset(self, batch_size):\n",
    "        dataset = self._create_dataset(self.val_indices, shuffle=True)\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        #dataset = dataset.map(lambda x: self._process_features((x), is_test=False))\n",
    "        return dataset\n",
    "\n",
    "    def get_test_dataset(self, batch_size):\n",
    "        dataset = self._create_dataset(self.all_indices, shuffle=False, is_test=True)\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=False)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        #dataset = dataset.map(lambda x: self._process_features((x), is_test=True))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "S7dsk2hHxkFi"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sequence_rmse_loss(y, y_hat, sequence_lengths, max_sequence_length=100, eps=1e-7):\n",
    "    # Ensure y is float32 for calculations\n",
    "    #y = tf.cast(y, tf.float32)\n",
    "    #print('casted', y.shape)\n",
    "    # Clip predictions to avoid NaNs in log calculations\n",
    "    square_loss = tf.square(y-y_hat)\n",
    "    #print('clipped', y_hat.shape)\n",
    "    # Create a sequence mask based on sequence lengths\n",
    "    sequence_mask = tf.cast(tf.sequence_mask(sequence_lengths, maxlen=max_sequence_length), tf.float32)\n",
    "    #print(\"mask created\", sequence_mask.shape)\n",
    "    # Apply the sequence mask to the  losses\n",
    "    # Compute the average log loss\n",
    "    avg_sq_loss = tf.reduce_sum(square_loss*sequence_mask) / tf.cast(tf.reduce_sum(sequence_lengths), tf.float32)\n",
    "    #print(\"reduced\")\n",
    "    return tf.sqrt(avg_sq_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UwpkzUEE8xtZ"
   },
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(self, lstm_size, **kwargs):\n",
    "        super(CustomModel, self).__init__(**kwargs)\n",
    "        self.lstm_size = lstm_size\n",
    "        self.n_components = 3\n",
    "        self.lstm_layer = tf.keras.layers.GRU(lstm_size, return_sequences=True, dropout=0.0)\n",
    "        self.dense1 = tf.keras.layers.Dense(50, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(self.n_components*3, activation='sigmoid')\n",
    "    def call(self,inputs):\n",
    "        user_id = inputs['user_id']\n",
    "        self.history_length = inputs['history_length']\n",
    "\n",
    "        order_size_history = inputs['order_size_history']\n",
    "        reorder_size_history = inputs['reorder_size_history']\n",
    "        order_number_history = inputs['order_number_history']\n",
    "        order_dow_history = inputs['order_dow_history']\n",
    "        order_hour_history = inputs['order_hour_history']\n",
    "        days_since_prior_order_history = inputs['days_since_prior_order_history']\n",
    "        next_reorder_size = inputs['next_reorder_size']\n",
    "        \n",
    "        \n",
    "        #print(product_embeddings.shape, aisle_embeddings.shape, department_embeddings.shape, is_none_float.shape, product_names.shape)\n",
    "\n",
    "        # Sequence data\n",
    "        order_dow_history_onehot = tf.one_hot(order_dow_history, 8)\n",
    "        order_hour_history_onehot = tf.one_hot(order_hour_history, 25)\n",
    "        days_since_prior_order_history_onehot = tf.one_hot(days_since_prior_order_history, 31)\n",
    "        order_size_history_onehot = tf.one_hot(order_size_history, 60)\n",
    "        reorder_size_history_onehot = tf.one_hot(reorder_size_history, 50)\n",
    "        order_number_history_onehot = tf.one_hot(order_number_history, 101)\n",
    "\n",
    "\n",
    "\n",
    "        order_dow_history_scalar = tf.expand_dims(tf.cast(order_dow_history, tf.float32) / 8.0, 2)\n",
    "        order_hour_history_scalar = tf.expand_dims(tf.cast(order_hour_history, tf.float32) / 25.0, 2)\n",
    "        days_since_prior_order_history_scalar = tf.expand_dims(tf.cast(days_since_prior_order_history, tf.float32) / 31.0, 2)\n",
    "        order_size_history_scalar = tf.expand_dims(tf.cast(order_size_history, tf.float32) / 60.0, 2)\n",
    "        reorder_size_history_scalar = tf.expand_dims(tf.cast(reorder_size_history, tf.float32) / 50.0, 2)\n",
    "        order_number_history_scalar = tf.expand_dims(tf.cast(order_number_history, tf.float32) / 100.0, 2)\n",
    "\n",
    "\n",
    "\n",
    "        outputs = tf.concat([\n",
    "            order_dow_history_onehot,\n",
    "            order_hour_history_onehot,\n",
    "            days_since_prior_order_history_onehot,\n",
    "            order_size_history_onehot,\n",
    "            reorder_size_history_onehot,\n",
    "            order_number_history_onehot,\n",
    "            order_dow_history_scalar,\n",
    "            order_hour_history_scalar,\n",
    "            days_since_prior_order_history_scalar,\n",
    "            order_size_history_scalar,\n",
    "            reorder_size_history_scalar,\n",
    "            order_number_history_scalar,\n",
    "        ], axis=2)\n",
    "\n",
    "\n",
    "        h = self.lstm_layer(outputs)\n",
    "        h = tf.concat([h,outputs], axis=-1)\n",
    "        h_final = tf.keras.layers.TimeDistributed(self.dense1, name='hidden_states')(h)\n",
    "        params = tf.keras.layers.TimeDistributed(self.dense2)(h_final)\n",
    "        \n",
    "        means, variances, mixing_coefs = tf.split(params, 3, axis=2)\n",
    "        mixing_coefs = tf.nn.softmax(mixing_coefs - tf.reduce_min(mixing_coefs, axis=2, keepdims=True))\n",
    "        variances = tf.exp(variances) + 1e-5\n",
    "\n",
    "        labels = tf.cast(tf.tile(tf.expand_dims(next_reorder_size, 2), [1, 1, self.n_components]), tf.float32)\n",
    "        n_likelihoods = 1.0 / (tf.sqrt(2*np.pi*variances)) * tf.exp(-tf.square(labels - means) / (2*variances))\n",
    "        nlls = -tf.math.log(tf.reduce_sum(mixing_coefs*n_likelihoods, axis=2) + 1e-10)\n",
    "\n",
    "        sequence_mask = tf.cast(tf.sequence_mask(self.history_length, maxlen=100), tf.float32)\n",
    "        nll = tf.reduce_sum(nlls*sequence_mask) / tf.cast(tf.reduce_sum(self.history_length), tf.float32)\n",
    "\n",
    "        # evaluate likelihood at a sample of discrete points\n",
    "        samples = tf.cast(tf.reshape(tf.range(25), [1, 1, 1, 25]), tf.float32)\n",
    "        means = tf.tile(tf.expand_dims(means, 3), [1, 1, 1, 25])\n",
    "        variances = tf.tile(tf.expand_dims(variances, 3), [1, 1, 1, 25])\n",
    "        mixing_coefs = tf.tile(tf.expand_dims(mixing_coefs, 3), [1, 1, 1, 25])\n",
    "        n_sample_likelihoods = 1.0 / (tf.sqrt(2*np.pi*variances)) * tf.exp(-tf.square(samples - means) / (2*variances))\n",
    "        sample_nlls = -tf.math.log(tf.reduce_sum(mixing_coefs*n_sample_likelihoods, axis=2) + 1e-10)\n",
    "\n",
    "        final_temporal_idx = tf.stack([tf.range(tf.shape(self.history_length)[0]), self.history_length - 1], axis=1)\n",
    "        final_states = tf.gather_nd(h_final, final_temporal_idx)\n",
    "        final_sample_nlls = tf.gather_nd(sample_nlls, final_temporal_idx)\n",
    "        final_states = tf.concat([final_states, final_sample_nlls], axis=1)\n",
    "        # final_states = tf.keras.layers.Lambda(lambda x: x, name='final_states')(final_states)\n",
    "        # final_predictions = tf.keras.layers.Lambda(lambda x: x, name='final_predictions')(final_predictions)\n",
    "        return {'final_states': final_states, 'loss':nll}\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        history_length = x['history_length']\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            # Pass both prediction and history_length to loss\n",
    "            loss = y_pred['loss']\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results['loss'] = loss\n",
    "        return results\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        history_length = x['history_length']\n",
    "\n",
    "        y_pred = self(x, training=False)\n",
    "        # Pass both prediction and history_length to loss\n",
    "        loss = y_pred['loss']\n",
    "\n",
    "        # Return metrics and val_loss\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results['loss'] = loss\n",
    "        return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLw5C3QeRRkV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "I_4Z20f7cZ6u"
   },
   "outputs": [],
   "source": [
    "reader = TFDataReader2('data')\n",
    "train_dataset = reader.get_train_dataset(128)\n",
    "val_dataset = reader.get_val_dataset(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "YVCMF4NJ_yF9"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints/cp-{epoch:04d}.ckpt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "0kauFm0OednL"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    # Early stopping to prevent overfitting\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # Model checkpoint to save best model\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models_gmm/best_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # Learning rate reduction on plateau\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=1,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # CSV logger\n",
    "    tf.keras.callbacks.CSVLogger(\n",
    "        'training_log.csv',\n",
    "        separator=',',\n",
    "        append=False\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        save_freq='epoch',\n",
    "        verbose=1\n",
    ")\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "element = None\n",
    "for input, output in train_dataset.take(1):\n",
    "  element = input\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "nqpEpl3WeLRW"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = CustomModel(\n",
    "        lstm_size=300,\n",
    "    )\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(0.001),\n",
    "    metrics=[\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qwll09q0W3Oa",
    "outputId": "249c4cc3-530b-41d2-8c27-92ac9525d4eb"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=None,\n",
    "    validation_steps=None,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,  # 0: silent, 1: progress bar, 2: one line per epoch\n",
    "    shuffle=True,  # Shuffle training data\n",
    "    max_queue_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('models_gmm/best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = reader.get_test_dataset(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict(test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pred_data/final_states_gmm.npy', outputs['final_states'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0WRXgTg7Tyu"
   },
   "outputs": [],
   "source": [
    "model.save_weights(\n",
    "            f'models/epoch_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DAh8Oma7XGu"
   },
   "outputs": [],
   "source": [
    "eval_results = model.evaluate(val_dataset, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGVb4PvP745N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "RecSys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
